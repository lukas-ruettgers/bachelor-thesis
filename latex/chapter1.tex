% !TEX root = thesis_ruettgers_lukas.tex
% From mitthesis package
% Version: 1.04, 2023/10/19
% Documentation: https://ctan.org/pkg/mitthesis

%% REFINEMENTS
% TODO: Alternatives for denote: denominate, designate
% TODO: Fix bib file. Wrap strings that should be capitalized by {}, e.g. {Mathematik}
% - In the context of supervised learning, we adopt the following terminology.

%% 2024/05/28: 6-7 pages of pure text
\chapter{Introduction} 
Think back to the moment where you and your classmates were taught elementary arithmetic on numbers.
Teachers across the world suggest their students to represent numbers in a particular numeral system, the decimal system.
In this vein, numbers are nothing but a sequence of symbols $a\in\{0,1,\dots,9\}$, and elementary arithmetic operations such as addition or multiplication can be briefly described as to inductively apply some simple symbol transformations over the symbol sequence.
Although the teacher gave you merely a few examples for small numbers, they contained sufficient information to describe this inductive algorithm, and enabled you to reproduce this algorithm for numbers beyond any witnessed scale.

%Now jump ahead to the moment when you were first introduced to the concept of prime numbers.
%You had already internalized the concept of divisibility by means of more fundamental concepts over the natural numbers.
The expedient lessons of this exemplary memory are twofold.
Firstly, it demonstrates that recursion is a simple, yet powerful mechanism to express unknown concepts with more fundamental ones and to generalize these concepts to unknown problem instances.
Secondly, it signifies that this generalization ability of humans rests on some kind of inductive bias that favours inferring \textit{simpler} concepts over more difficult ones.
That way, the amount of information that the teacher needed to convey in terms of definitions and examples to let its students entirely comprehend the concept is related to some notion of the \textit{simplicity} of this concept.

%% RELATION OF COMPRESSION TO IN-DISTRIBUTION-GENERALIZATION
% - Benefit: far tighter, distribution-free bounds than classical statistical learning theory.
% - Caveat: Still i.i.d. assumption(!)

\section{Purpose and contributions}
In the setting of supervised learning, we also provide a learning algorithm examples in terms of functional mappings $(x,y)$ to eventually guarantee that it must detect the correct function that produces these mappings.

This thesis unravels how such a simplicity bias that draws on Kolmogorov complexity could enable the learnability of any computable function with finite information-theoretic resources, and even allows to undercut the sample sizes that are usually required to learn such functions in other learnability frameworks, such as the PAC model.
At the same time, it points up limitations in our models, learning algorithms, and learnability conditions that hinder us from learning simple functions.
Using the example of certain recursive functions, we\footnote{If the author uses the pronoun ``we'' in the subsequent work, he jointly refers to the readership and himself.} first demonstrate how feed-forward neural networks and any other model with a \textit{non-recursive structure} can not even express functions that are however simple to describe given what functions --- activation functions, arithmetic operations, constants, logical expressions, etc. --- they have access to.
These restraints of their representation class hence bias \textit{against} some simple functions.
On the other hand, learnability guarantees such as classical ones that are based on the Rademacher complexity or VC dimension of hypothesis classes rest on some limitations of the hypothesis class.
By quantifying the information in a dataset about the underlying function in terms of Kolmogorov complexity, such restraints however disappear, as the hypothesis class may now admit \textit{any} partial computable function and still render finite information sufficient to learn the true function.
Since Kolmogorov complexity $K$ is incomputable, this thesis finally touches on viable approximations of this quantity.
Although compression algorithms are frequently proposed in practice to estimate the simplicity within data, this thesis corroborates that any of the contemporary compression algorithms are unable to compress simple regularities in strings $v$ to an arbitrary scale, and therefore can not even yield approximate theoretical guarantees on the \textit{order} between the Kolmogorov complexity of two strings, $K(v)<K(w)$, even if we allow for any exponential, multiplicative, and additive approximation offsets.

\section{Content structure}
To begin with, Chapter \ref{chap:preliminaries} provides essential preliminaries and contextualizes our work in light of prior research. 
While Section \ref{sec:domain-generalization} presents pertinent work in the field of domain generalization and indicates remaining intricate problems, Section \ref{sec:kolmogorov-complexity} provides a comprehensive introduction to Kolmogorov complexity.

Subsequent to that, Chapter \ref{chap:models-optimization-kolmogorov} addresses the expressive limitations of non-recursive models and the discriminative flaws of optimization objectives.
To that end, Section \ref{sec:non-recursive-models} first rigorously defines \textit{non-recursive models}.
Then, Section \ref{sec:recursive-completion} introduces a class of recursive, simple functions that such models cannot express.
While the inexpressivity result is proven in Section \ref{sec:expressive-limits-non-recursive-models}, Section \ref{sec:uniform-simplicity-recursive-completion} shows that these functions have a constantly low Kolmogorov complexity given the functions these models have access to.
Finally, Section \ref{sec:optimization-objectives-lack-simplicity-bias} demonstrates that infinitely many functions that could be expressed by these non-recursive models achieve the optimal score according to standard optimization objectives like Empirical Risk Minimization or even Invariant Risk Minimization, because these models could essentially memorize the training data.
Thereon, it exemplifies how the discriminative capability of Kolmogorov complexity renders all these functions suboptimal for sufficient large finite datasets, and hence allows to learn the true recursive function.

Generalizing this insight, Chapter \ref{chap:sufficient-information-learnability} formulates sufficient information-theoretic conditions that allow to learn the overarching hypothesis class of computable functions, but still remain optimal, in the sense that less information would in general not allow to learn the respective function.
After Section \ref{sec:sample-size-does-not-avail-learnability} exemplifies that it is neither possible nor meaningful to merely condition learnability of computable functions on the number of samples in the dataset, Section \ref{sec:quantify-functional-information} alternatively elaborates on quantifying the information that datasets convey about the functions that could have generated it in terms of Kolmogorov complexity.
To that effect, it substantiates that weaker formulations of this information based on Kolmogorov complexity violate desirable properties, and thereby concludes with the ultimate formulation, coined \textit{functional information}.
Thereafter, it elucidates how this functional information smooths the way to the aforementioned learnability conditions, and juxtaposes them to prior works in learnability and compression. In particular, it exemplifies on the hypothesis class of $d$-dimensional parity functions that the additional discriminative power of the incorporated simplicity bias allows to learn parity functions with less than $d$ samples, given their Kolmogorov complexity is relatively small.
Finally, Section \ref{sec:functional-inforamtion-simplicity-bias} illustrates how an oracle for functional information could be used to realise a simplicity bias in practice.

Although Kolmogorov complexity is incomputable, a simplicity bias merely requires an approximation that could yield some approximate guarantee on the \textit{order} between the Kolmogorov complexity of two strings $K(v)<K(w)$.
However, Chapter \ref{chap:compression-kolmogorov-complexity} addresses why compression algorithms, which are frequently proposed as approximations of Kolmogorov complexity, do not even yield such approximate guarantees.
For this purpose, Section \ref{sec:limpel-ziv-welch-algorithm} uses the Lempel-Ziv-Welch algorithm as a classical example of compression algorithms to demonstrate their limitations in compressing strings that have been generated by simple algorithms.
After corroborating the arbitrary compression power of Turing Machines in Section \ref{sec:compressibility}, Section \ref{sec:unbounded-order-inconsistencies} instantiates that such compression algorithms can not yield such guarantees for any exponential, multiplicative, and additive approximation offsets.

Finally, we summarize the above results and their impact in Chapter \ref{chap:conclusion} and raise future research questions.
