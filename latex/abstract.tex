% !TEX root = thesis_ruettgers_lukas.tex
% From mitthesis package
% Version: 1.01, 2023/06/19
% Documentation: https://ctan.org/pkg/mitthesis
%
% The abstract environment creates all the required headers and footnote. 
% You only need to add the text of the abstract itself.
%
% Approximately 500 words or less; try not to use formulas or special characters
% If you don't want an initial indentation, do \noindent at the start of the abstract

% TODO: Write Abstract
\noindent
The reasonable maximum demand on out-of-distribution generalization is that the learning algorithm should infer the \textit{simplest} function that remains consistent with the observed data.
%Out-of-distribution generalization requires agents to induce from their experience to unseen environments. 
%However, the reasonable maximum demand on inductive inference is to detect and infer the simplest underlying function that remains consistent with the observed data.
% TODO: Maybe replace information-theoretic by descriptive
Kolmogorov complexity intuitively formalizes the notion of simplicity from an information-theoretic viewpoint, as it associates the simplicity of a function with the description length of the shortest program that can compute it.
% Kolmogorov complexity intuitively formalises the descriptive complexity of a function, as it considers the length of the shortest program description that can produce it.
In light of this descriptive complexity, this thesis addresses limitations in our model classes, optimization techniques, and statistical learnability conditions that impede learning simple functions.
% In light of this descriptive complexity, current models and optimization techniques still lack the ability to learn recursive functions which are known to comprise simple yet powerful patterns.\\
% This thesis puts forward three arguments to illustrate these limitations.

Firstly, it demonstrates that models with a \textit{non-recursive structure} such as feed-forward neural networks are incapable of even expressing a class of functions that yet preserve a constantly low Kolmogorov complexity.
% To that end, it holistically considers what functions any such model could possibly express and defines a class of recursive functions that are inexpressible for these models yet preserve constantly low Kolmogorov complexity.
% To abstract from their specific architectures, it analyses this entire class of models from the viewpoint of first-order logical terms to consider what functions they could possibly express. 
% Upon this framework, it then defines a powerful class of recursive functions that yet preserve low Kolmogorov complexity.
% Then, it proposes a slight extension of first-order logic to recursive first-order logic and defines a class of decision problems over countably infinite alphabets upon it that capture powerful concepts and yet preserve low Kolmogorov complexity.
% And even if models were powerful enough to express such recursive patterns, ...
% This function class showcases another limitation that is independent of model expressivity and rather lies in the optimization formulations.
On the other extreme, models that allowed to express \textit{any partial computable function} would render learnability from finite datasets practically impossible with standard learning algorithms.
This is because present learnability conditions in theory either restrict themselves to exemplary model or hypothesis classes \cite{ahuja2021invariance,arjovsky2019invariant}, or rely on overly conservative assumptions such as the ubiquitous \textit{i.i.d.} assumption \cite{paccagnan2024pick,campi2023compression}.
% like ERM, IRM

In this setting of overarching model expressivity, this work instead substantiates how incorporating Kolmogorov complexity as a simplicity bias into the optimization objective function carves the way to formulate general distribution-free, both necessary and sufficient information-theoretic conditions that a dataset must satisfy to learn \textit{any} partial computable function, and exemplifies how such an inductive bias can further reduce the sample size that is usually required for classical learnability guarantees.
To that end, it proposes to directly draw upon Kolmogorov complexity to quantify the information that finite datasets convey about the functions that could have possibly generated it.
%, and illustrates how this quantity --- coined \textit{functional information} --- could realise simplicity biases in practice.

% that even dedicated domain generalization optimization objectives like Invariant Risk Minimization \cite{arjovsky2019invariant} still fail to identify the true hypothesis $h$ given finite datasets, because they do not favour simpler concepts over more complex ones.
% When however incorporating Kolmogorov complexity as a simplicity bias into the optimization objective, any dataset $D$ beyond a certain size will ensure that all non-recursive functions that still remain consistent with $D$ have a higher Kolmogorov complexity than $h$.
% Instead, it will assign the same optimal objective function value to infinitely many other hypotheses, because it does not favour simpler concepts.

Because Kolmogorov complexity is incomputable in general, compression algorithms are typically proposed as viable approximations \cite{cilibrasi2005clustering}. 
However, for \textit{any} choice on the encoding of Turing Machines, contemporarily employed compression algorithms such as the Lempel-Ziv-Welch compression can not even yield approximate guarantees about the order between the Kolmogorov complexity of two binary strings, because such compression algorithms do not keep up with the compression power of Turing Machines.
% TODO: Discrete parametrization of total computable functions is misleading.
% - (Prefix-free) encoding of Turing Machines $\not\equiv$ discrete parametrization of total computable functions
% TODO: Citation for the generalization improvement of regularization.
%Although regularization provably improves generalization within particular parametrized function classes, this thesis finally corroborates that for any choice on the encoding of Turing Machines, regularization can not even yield approximate guarantees about \textit{functional information}, because there are arbitrarily large order inconsistencies between the Kolmogorov complexity and the norm of binary vectors.
% For restricted parametrized function classes, regularization  is a widely used tool that controls the magnitude of the parameters and in this way provably improves generalization guarantees for functions realizable within the representation class.

% However, we show that regularization over any vector norm that would be practically feasible for regularization does not yield any simplicity guarantees for the function it encodes. 
% For arbitrarily large multiplicative and additive offset, there are bidirectional inconsistencies in the order that any such norm and Kolmogorov complexity induces on discrete-valued sequences.
% WORDING: compensation, offset (Ausgleich)

% The final argument concerns our theoretical approaches to yield generalization error bounds for hypothesis classes that are based on their complexity. 
% Finally, this work proves that if we regard functions by their Kolmogorov complexity instead of complexity measures employed in classical learning theory, it is not meaningful to condition learnability merely on the number of samples.
% Rademacher Complexity, VC dimension 
% Instead, it proposes to quantify such information-theoretic conditions in terms of Kolmogorov complexity too, and argues how it might empower generalization analysis beyond the omnipresent i.i.d. assumptions.




%%%%%%%%%%%%%%%%%% Alternative formulations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% While recursion is a powerful tool for compressing algorithmic behaviour, the hardness of learning recursive descriptions has impeded its integration into modern machine learning pipelines.

% It emphasizes the efficacy of learning recursive algorithmic descriptions for reasonable behaviour outside of the training distribution, and showcases drawbacks in existing frequently employed models and optimization objectives in capturing elementary recursive patterns.
