% !TEX root = thesis_ruettgers_lukas.tex
% From mitthesis package
% Version: 1.01, 2023/07/04
% Documentation: https://ctan.org/pkg/mitthesis

\chapter{Conclusion}
\label{chap:conclusion}
%% DISCUSSION
Our limitations to let machines learn simple functions are yet manifold.
First of all, recursion is a powerful, yet simple computational concept that constitutes a multitude of simple functions.
However, feed-forward neural networks and other models with non-recursive structure can not even express some simple recursive functions, and hence disregard a broad range of relevant patterns in nature.
At the same time, learnability conditions for standard optimization objectives like ERM or IRM rely on some constraints what the representation class can express.
Incorporating the discriminative power of Kolmogorov complexity as a simplicity bias into the optimization objective would however guarantee that at least all total computable functions are learnable with finite statistical information, even in the overarching setting where all partial computable functions are admitted to the representation class, a scenario for which classical PAC learnability based on the VC dimension or Rademacher complexity bounds could not yield any guarantee.
Moreover, this additional discriminative power can also reduce the number of samples that are required to information-theoretically guarantee learnability below the typical thresholds in the PAC model, such as was shown for $d$-dimensional parity functions, where less than $d$ samples can suffice to render all hypotheses with a lower Kolmogorov complexity suboptimal.

Our distribution-free setting where learnability is merely conditioned on the functional information in the dataset substantiates that limited training distributions that do not cover the entire support of possible instances can certainly suffice to learn any computable function, as long as the samples from this limited distribution are sufficiently informative that they cannot be losslessly compressed below the shortest description length of the true function.

Learning is therefore inseparably linked to compression, because learning the true function can be regarded as identifying the shortest compression that encompasses sufficient information to generate the labels when given the samples.
However, compression algorithms do not keep up with the compression power of Turing Machines, and therefore can not even yield approximate guarantees on the \textit{order} between the Kolmogorov complexity of two strings.

Explicitly realising such an approximation of at least the ordering that Kolmogorov complexity induces seems hard and directly related to the learning problem itself. 

\section{Future work}
In practice, viable heuristics that indirectly pursue the simplicity bias of Kolmogorov complexity might however already take advantage from its aforementioned benefits.
The statements in Chapter \ref{chap:sufficient-information-learnability} hold for any possible prefix-free encoding of Turing Machines, since they only rest on the property that there are only finitely many functions with a lower complexity.
Although the heuristic simplicity biases we derive in practice might not precisely agree with the intuitive information-theoretic notion of simplicity that Kolmogorov complexity over the standard Turing Machine encodings expresses, such heuristics might advance our theoretical toolbox to derive out-of-distribution generalization guarantees under weaker statistical assumptions.
Instead of encoding Turing Machines, such simplicity biases could also be directly defined over model classes such as neural networks.
While the complexity of multi-layer neural networks is usually merely quantified in terms of their layer width, depth, or the norm of its weights, desirable simplicity biases could further consider regularities in the computation graph of the neural network.
As this thesis put forward the general benefit of simplicity biases independent of the exact order between functions, having such a specific simplicity bias at hand would allow to quantify more precise information-theoretic learnability guarantees.

Moreover, the role of recursion in empowering model classes to express simple but omnipresent patterns should not be neglected.
However, the problems that bar the way from learning recursive algorithms still remain uncharted waters.
Firstly, the error back-propagation on which gradient descent draws upon can not be directly transferred to models where the structure of the computation graph is not fixed but also subject to optimization.
On the contrary, recurrent neural networks have a \textit{fixed} recursive structure. When they are trained to fit a time series, they are provided with a learning signal at each step in time to optimize parameters within this fixed structure.
But it is the recursive structure itself that needs to be learned when models shall express a more powerful range of simple recursive patterns.
However, it is hard to parametrize this model structure in a way that a function that expresses this model class is differentiable over these parameters.
Therefore, it is yet unclear how alterations in the structure shall be guided by the value of the loss function. 

Although these challenges are hard, they seem impossible to bypass on our quest for empowering machines to learn simple functions and establishing guarantees about their generalization behaviour.

%The steps towards such a powerful simplicity bias must be taken gradually and carefully.
%One of these next steps could include to further study the role of recursion to enable models to express simple functions.
%Although feed-forward networks alone might not exhibit this ability, modern architectures such as attention models notably advanced the expressive power and empirical generalization ability of large language models \cite{brown2020language}.
%Signs of simplicity biases within these models would further corroborate the theoretical foundation for these empirical capabilities.
%
%At the same time, bestowing compression algorithms with recursive procedures to enable them to take further advantage of recursive patterns in the data might scale their compression ratio limits.
%Although these limits might yet not yield approximate guarantees on the order induced by Kolmogorov complexity in the generality in which it was studied here, it might certainly be an important step towards it.
%
%%Finally, it might be interesting to characterize the Kolmogorov complexity of hypothesis class by means of \textit{symmetries} between the functional description of members.