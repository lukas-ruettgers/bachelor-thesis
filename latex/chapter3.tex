% !TEX root = thesis_ruettgers_lukas.tex
% From mitthesis package
% Version: 1.01, 2023/07/04
% Documentation: https://ctan.org/pkg/mitthesis

\chapter{Models and Optimization that are Mindless of Simplicity}
\label{chap:models-optimization-kolmogorov}
In this chapter, we consider learning problems of the form $h:\mathbb{N}^k\to\mathbb{N}$ for some arbitrary $k\in\mathbb{N}$.
To cover the entire range of signed, finite precision floating point numbers in computing architectures and thus all functions that neural network can express in practice, we will later show how our results extend from $\mathbb{N}$ to $\mathbb{Z}$.
Figure \ref{fig:overview-chap-3} provides a holistic roadmap for the line of reasoning that this chapter trails, which we now explain in more detail.
\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[every text node part/.style={align=center}, font=\small]
			
			%%% HEADERS %%%%%%%%%%%%%%%%%%%%%%%%
			\node[xshift=40pt,font=\large] (models) at (0,0) {\textbf{Models} };
			\node[right=100pt of models,font=\large] (simplicity) {\textbf{Simplicity}};
			\node[right=80pt of simplicity,font=\large] (algorithms) {\textbf{Algorithms}};
			
			%%% MODELS %%%%%%%%%%%%%%%%%%%%%%%%
			\node[below = 20pt of models.south] (func-set){function set\\ $\tau=\{f_1,\dots,f_j\}$};
			\draw[->, dashed, rwth-storm] 
			([xshift=0pt]models.south) 
			to [out=240,in=120] ([xshift=0pt]func-set.north); 
			\node[above=5pt of func-set.north,font=\tiny,rwth-storm,xshift=-30pt]{have access to}; 
			
			
			
			
			%% ARROW FINITE CONCATENATIONS %%%%%%%%%%%%%%%%%%%%%%%%
			
				\node[below = 35pt of func-set] (non-rec-funcs){non-recursive functions $\mathcal{F}_\tau$};
				\draw[->, dashed,rwth-storm] 
				([xshift=0pt]func-set.south) 
				to [out=240,in=120] ([xshift=0pt]non-rec-funcs.north); 
				\node[
				above=10pt of non-rec-funcs.north,
				xshift=-30pt,
				font=\tiny, rwth-storm
				] (finite-concats) {all finite \\concatenations};
			
			
			%% ARROW REPRESENTATION CLASS WITHIN %%%%%%%%%%%%%%%%%%%%
			
				\node[below = 25pt of non-rec-funcs] (non-rec-models){non-recursive models over $\tau$};
				\draw[->, dashed,rwth-storm] 
				([xshift=0pt]non-rec-models.north) 
				to [out=120, in=240](non-rec-funcs.south);
				\node[
				below = 5pt of non-rec-funcs.south,
				font=\tiny,
				xshift=-30pt, rwth-storm] 
				(repr-class-within) {representation \\ class within};
				
				\node[below=-2pt of non-rec-models.south, font=\tiny, gray] (func-set-info) 
				{e.g. multi-layer neural networks};
				
			
			
			
			%% ARROW CANNOT EXPRESS %%%%%%%%%%%%%%%%%%%%%
			
				\node[below=15pt of simplicity.south] (rec-completion) {recursive completion $f_\tau^{\lozenge}$};
				\draw[->,rwth-purple] (non-rec-funcs.north east) to 
				[out=60, in=230](rec-completion.south west);
				\node[
				above= 20pt of non-rec-funcs.north east,
				font=\tiny,
				xshift=0pt,rwth-purple
				] (cannot-express) {cannot \\ express};
		
			
			
			%% ARROW CONSTANTLY LOW KOLMOGOROV %%%%%%%%%%%%%%%%%%%%%
			
				\node[below=30pt of rec-completion] (kolmogorov-complexity) {Kolmogorov complexity};
				\draw[->,rwth-purple]
				([xshift=10pt]rec-completion.south)
				to [out=300, in=60]([xshift=10pt]kolmogorov-complexity.north);
				\node[
				below=5pt of rec-completion, 
				font=\tiny,
				xshift=40pt,rwth-purple] (constantly-low) {constantly low \\ given $\tau$};
			
			
			%%% Algorithms %%%%%%%%%%%%%%%%%%%%%%%%%%%%
			
			%% ARROW INFINITELY OPTIMAL %%%%%%%%%%%%%%%%%%%%%
		
				\node[below = 125pt of algorithms.south] (irm) {ERM/IRM};
				\draw[<-,rwth-purple]
				(irm.west) to [out=180, in=320] ([xshift=40pt]non-rec-funcs.south);
				\node[
				left= 50pt of irm.west,
				font=\tiny,
				yshift=-15pt,rwth-purple
				] (infinitely-many-consistent) {for any finite dataset generated from $f_\tau^{\lozenge}$ \\ infinitely many remain consistent and hence optimal};
			
			
			%% ARROW SIMPLICITY BIAS %%%%%%%%%%%%%%%%%%
				\node[
				draw,
				dashed,
				rectangle, 
				right=10pt of kolmogorov-complexity.south east, 
				yshift=-20pt] (simplicity-bias){Simplicity bias \\ + ERM/IRM};
				\draw[->, dashed,gray]
				(irm.north) to [out=90, in=0](simplicity-bias.east);
				\draw[->, dashed,gray]
				(kolmogorov-complexity.east) to [out=0, in=90](simplicity-bias.north);
				
				%% ARROW SUBOPTIMAL %%%%%%%%%%%%%%%%%%%%%%
				\draw[->,rwth-purple]
				(simplicity-bias.west) to [out=180, in=350](non-rec-funcs.east);
				\node[
				left= 10pt of simplicity-bias.west,
				font=\tiny,
				yshift=-10pt,rwth-purple
				] (render-suboptimal) {renders all consistent functions suboptimal\\ for sufficiently large datasets};
		\end{tikzpicture}
	\end{center}
	\caption[Overview of the line of reasoning in Chapter \ref{chap:models-optimization-kolmogorov}.]{Holistic overview of the line of reasoning in Chapter \ref{chap:models-optimization-kolmogorov}.}
	\label{fig:overview-chap-3}
\end{figure}
When we construct mathematical models to learn predictors for such functions $h$, there is an underlying finite set of operations or functions $\tau=\{f_1,\dots,f_j\}$ that serve as \textit{building blocks} of our model.
In practice, these are the functions we eventually use in our source code to express a model in a certain programming language.
For neural networks, typical building block functions include arithmetic operations like addition or multiplication, activation functions like the sigmoid function $\sigma(x)=\frac{1}{1+e^{-x}}$, ReLU, ELU, or $\tanh$, constants $c$, and logical expressions like the equality or inequality $=,<$, and logical conjunctions such as $\land$ and $\lor$.

In the case of feed-forward neural networks $M_\theta$, our building block functions $f_i$ are merely concatenated in a way that does not depend on the input $x$.
In particular, the number of layers and the values of the parameters are fixed and do not change according to the input $x$.
More formally, the function $f_{\theta}$ that $M_\theta$ computes is equivalent to a finite concatenation of the building block functions $f_1,\dots,f_j$.
Such multi-layer neural networks are well-known instances of a model class that we will later rigorously define as \textit{non-recursive models}.
Informally, non-recursive models over $\tau$ can merely express finite concatenations of the building block functions in $\tau$.

The demonstrations of this chapter are twofold.
Firstly, this chapter puts forward that there are simple functions that these non-recursive models are not able to express.
To that end, Theorem \ref{theorem:recursive-completion-not-non-recursive-expressible} states that models with a non-recursive structure always have a certain class of recursive functions $h_{\tau}$ they are incapable of even expressing, no matter what building block function set $\tau$ they were given access to, as long as the functions in $\tau$ satisfy conditions that frequently hold in practice. 
At the same time, Theorem \ref{theorem:recursive-completion-kolmogorov-complexity} asserts the functions $h_{\tau}$ of this very class preserve a constantly low Kolmogorov complexity given $\tau$, which allows us to conclude the aforementioned statement.

On the other hand, the ability of standard optimization objectives to learn the true function also relies on limitations of the model class.
In the limit of expressive power where \textit{any} total computable function could be expressed by a model, any finite dataset would have infinitely many functions that remain consistent with it, hindering even optimization objectives that are dedicated to domain generalization like IRM from identifying the true function.

%Irrespective of what a model can express, this section additionally exhibits that even dedicated domain generalization optimization objectives like IRM are indifferent about the simplicity of a hypothesis $h$, and therefore fail to identify the true hypothesis $h^{*}$ even if all other consistent hypotheses have a higher Kolmogorov complexity.
Using the example of the aforementioned class of recursive functions $h_\tau$, Theorem \ref{theorem:surpass-kolmogorov-complexity-threshold} however illustrates how incorporating Kolmogorov complexity as a simplicity bias into the optimization objective would ensure that for sufficiently large datasets, every function that a corresponding non-recursive model could possibly express with $\tau$ obtains a higher Kolmogorov complexity than the true function $h_\tau$.
% However, for sufficiently large datasets, Theorem \ref{theorem:surpass-kolmogorov-complexity-threshold} points up that all of these functions will obtain a higher Kolmogorov complexity than the true function $h$, which demonstrates how domain generalization could profit from extending optimization methods by a simplicity bias.

By exemplifying the discriminative power of this simplicity bias, these results therefore encourage to study how Kolmogorov complexity paves the way to general, yet nearly optimal learnability conditions for hypothesis classes as large as the class of all partial computable functions, which we undertake later in Chapter \ref{chap:sufficient-information-learnability}.

\section{Non-recursive models}
\label{sec:non-recursive-models}
On our way to provide a rigorous definition of non-recursive models, we first characterize what it means for a function to be non-recursive.

%% ASSUMPTIONS OF OUR PROOFS
% Besides the countable infinity of $\mathbb{N}$ and their finite number density in any bounded interval, the formal arguments only assume a smallest element and a notion of boundedness and monotonicity on the underlying operand set, and hence necessitate only little adjustments to extend them to the set of finite strings $\{0,1\}^{*}$.

%% EXTENSIONS OF OUR PROOFS
% What about changing the recursive completion to taking only the absolute magnitude of each function $\lvert f_i(n) \rVert$? 
% - This value would still be bounded by $0$, but does it really preserve a meaningful notion of monotonicity?

%% Under which conditions do our lemmas transfer to negative numbers?
% - Convergence in both directions need to be considered.
% The results therefore transfer directly to other countably infinite operand sets like the set of finite strings $\{0,1\}^{*}$ or the non-negative rational numbers $\mathbb{Q}^{\geq 0}$.

%% Finite concatenations
\begin{definition}[Non-recursive functions]
	\label{def:non-recursive-functions}
	Given a finite set of functions $\tau=\{f_1,\dots,f_j\}$ such that for each $1\leq i\leq j$, $f_i:\mathbb{N}^{k_{i}}\to\mathbb{N}$ is a $k_i$-ary function over $\mathbb{N}$ for some $k_i\in\mathbb{N}$.
	
	We inductively define the set of non-recursive functions over $\tau$, denoted $\mathcal{F}_{\tau}$.
	
	Beforehand, take $\operatorname{VAR}:=\{n_i\mid i\in\mathbb{N}\}$ as the set of variable symbols that functions in $\mathcal{F}_{\tau}$ can take as distinct input variables.
	\begin{enumerate}
		\item \textbf{Base case:} For any variable symbol $n_i$, the identity function $I\in \mathcal{F}_{\tau}$, where $I(n_i)=n_i$ for all $n_i\in\mathbb{N}$.
		\item \textbf{Induction step:} Let $f\in\tau$ be a function of arity $K\in\mathbb{N}$. Let $g_1,\dots,g_K\in\mathcal{F}_{\tau}$ be arbitrary non-recursive functions, where $g_i$ has arity $k_i\in\mathbb{N}$. 
		
		Let $i_1,\dots,i_m\in\mathbb{N}$ be the distinct indices of the variable symbols $n_i$ that occur as an input variable symbol in at least one of the functions $g_1,\dots,g_k$. 
		Abbreviate these $m$ variable symbols as a vector $\mathbf{n}:=(n_{i_1},\dots,n_{i_m})$. Accordingly, let $\mathbf{n}_i$ be the tuple of the variable symbols $n_{i_j}\in \mathbf{n}$ that occur in $g_i$, ordered by $j$. 
		
		Then, the function $h:=(f\circ(g_1,\dots,g_k))\in \mathcal{F}_{\tau}$, where $h(\mathbf{n}):=f(g_1(\mathbf{n}_1),\dots,g_K(\mathbf{n}_K))$ for all $\mathbf{n}\in\mathbb{N}^m$.
	\end{enumerate}
\end{definition}
Non-recursive functions over $\tau$ are hence nothing but finite concatenations of functions in $\tau$.
For example, if the function set $\tau$ consists out of the constants $f_0(n_i)=0,f_1(n_i)=1$, and the addition function $f_{+}(n_i,n_j)=n_i+n_j$, the non-recursive functions over $\tau$ are nothing but the class of linear functions over $\mathbb{N}$, $\mathcal{F}_{\tau}=\{f:f(n)=an+b\mid a,b\in\mathbb{N}\}$.

As we study the expressive power of models, it suffices to identify models with the representation class $\mathcal{M}$, that contains all functions they can realize.
Therefore, we call a model non-recursive on $\tau$ if its representation class $\mathcal{M}\subset \mathcal{F}_{\tau}$.
In other words, if a model is non-recursive on $\tau$, any function $g$ that this model can realize is equivalent to a certain finite concatenation of functions in $\tau$.

%In the following, we abbreviate the arity of a function $f$ by $\operatorname{ar}(f)$.

The inductive definition of $\mathcal{F}_{\tau}$ furnishes a fit occasion to prove statements over this function class in an analogously inductive manner.
For that reason, we establish the notion of the \textit{depth} of a non-recursive function as its maximum concatenation depth.

\begin{definition}[Depth of non-recursive functions]
	\label{def:non-recursive-functions-depth}
	We inductively define the depth of non-recursive functions $\operatorname{dep}:\mathcal{F}_{\tau}\to\mathbb{N}$ as follows:
	\begin{enumerate}
		\item \textbf{Base case:} The identity function $I$ has depth $\operatorname{dep}(I):=0$. 
		\item \textbf{Induction step:} Let $f\in\tau$ be a function of arity $K\in\mathbb{N}$. Let $g_1,\dots,g_K\in\mathcal{F}_{\tau}$ be arbitrary non-recursive functions. Then, the function $h:=(f\circ(g_1,\dots,g_k))$ has depth $\operatorname{dep}(h):=1+\max_{1\leq i\leq k}\operatorname{dep}(g_i)$.
	\end{enumerate}
\end{definition}

Moving on, the monotonicity of non-recursive functions is defined in the standard way.
\begin{definition}[Monotonous non-recursive functions]
	\label{def:non-recursive-functions-monotonous}
	Let $f\in \mathcal{F}_{\tau}$ be an arbitrary $k$-ary non-recursive function with $k\geq 1$.
	
	We call $f$ monotonously increasing if for any $1\leq i\leq k$, for any $n_1,\dots,n_k\in \mathbb{N}$, and any $a,b\in\mathbb{N}$ with $a<b$, it holds that
	\begin{align*}
		f(n_1,\dots,n_{i-1},a,n_{i+1},\dots,n_k) \leq f(n_1,\dots,n_{i-1},b,n_{i+1},\dots,n_k).
	\end{align*}
	
	If the inequality holds strictly, we call $f$ strictly monotonously increasing.
	
	The definition of (strictly) monotonously decreasing functions is symmetric.
\end{definition}

\section{Recursive completion}
\label{sec:recursive-completion}
Non-recursive functions as by Definition \ref{def:non-recursive-functions} have a concatenation structure that is uniform across all inputs. 
Conversely, the concatenation structure of recursive functions might certainly depend on the input.

% To show that recursive functions constitute an indispensable class of simple functions that cannot be expressed by non-recursive models, we construct functions that recursively apply certain operations --- first-order logical terms in our case --- for a number of times that is not fixed but depends on the input.

To formally exemplify recursive functions and contrast them to non-recursive ones, we first introduce the \textit{recursive concatenation} of an arbitrary function, which is nothing but the concatenation of a function with itself.
\begin{definition}[Recursive concatenation]
	\label{def:recursive-concatenation}
	Let $f:\mathbb{N}^k\to\mathbb{N}$ be an arbitrary function over $\mathbb{N}$ with arity $k\geq 1$.
	First, denote by $f^{-}(n):=f(\underbrace{n,\dots,n}_{k \text{ times}}),n\in\mathbb{N}$ the reduction of $f$ to a unary function that copies the single input $n$ to all $k$ input variables.
	
	We inductively define the $m$-wise recursive concatenation of $f$ as
	\begin{align}
		f^{(0)}(n)&:=n, && n\in\mathbb{N}\\
		f^{(m+1)}(n)&:=f^{-}(f^{(m)}(n)),&& n,m\in\mathbb{N}.
	\end{align}
\end{definition}
Using the example of a $2$-ary function $f$, Figure \ref{fig:recursive-concatenation} visualizes the functional composition of the recursive concatenation $f^{(m)}$ by its term tree.
\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[every text node part/.style={align=center}, font=\normalsize]
			%% F1
			\node[] (f1) at (0,0){$f^{(1)}(n):$};
			
			%% F1 UPPER LAYER
			\node[right=10pt of f1.east,yshift=-5pt] (f1up) {$f(\;\cdot\;,\;\cdot\;)$};
			
			%% F1 LOWER LAYER
			\node[below = 20pt of f1up,xshift=-25pt] (f1lowl){$n$};
			\node[below = 20pt of f1up,xshift=25pt] (f1lowr){$n$};
			
			\draw[-] ([xshift=-5pt]f1up.south) -- (f1lowl.north);
			\draw[-] ([xshift=10pt]f1up.south) -- (f1lowr.north);
			
			
				%%% F2
				\node[right=100pt of f1.east] (f2) {$f^{(2)}(n):$};
				
				%% F2 UPPER LAYER
				\node[right=10pt of f2.east,yshift=-5pt] (f2up) {$f(\;\cdot\;,\;\cdot\;)$};
				
				%% F2 MIDDLE LAYER
				\node[below = 20pt of f2up,xshift=-35pt] (f2midl){$f(\;\cdot\;,\;\cdot\;)$};
				\node[below = 20pt of f2up,xshift=35pt] (f2midr){$f(\;\cdot\;,\;\cdot\;)$};
				
				\draw[-] ([xshift=-5pt]f2up.south) -- (f2midl.north);
				\draw[-] ([xshift=10pt]f2up.south) -- (f2midr.north);
				
				%% F2 LOWER LAYER
				% LEFT
				\node[below = 20pt of f2midl,xshift=-25pt] (f2lowll){$n$};
				\node[below = 20pt of f2midl,xshift=25pt] (f2lowlr){$n$};
				
				\draw[-] ([xshift=-5pt]f2midl.south) -- (f2lowll.north);
				\draw[-] ([xshift=10pt]f2midl.south) -- (f2lowlr.north);
				
				% RIGHT
				\node[below = 20pt of f2midr,xshift=-25pt] (f2lowrl){$n$ };
				\node[below = 20pt of f2midr,xshift=25pt] (f2lowrr){$n$};
				
				\draw[-] ([xshift=-5pt]f2midr.south) -- (f2lowrl.north);
				\draw[-] ([xshift=10pt]f2midr.south) -- (f2lowrr.north);
			
		\end{tikzpicture}
	\end{center}
	\caption[Recursive concatenation of a $2$-ary function.]{Term tree of the recursive concatenation of a $2$-ary function $f^{(m)}$ for $m=1,2$.}
	\label{fig:recursive-concatenation}
\end{figure}
If functions $f$ over $\mathbb{N}$ are strictly monotonously increasing and have an arity larger than $1$, the function values of their recursive concatenations $f^{(m)}(n)$ must grow exponentially fast in the number of concatenations $m$, as the following Lemma shows.

\begin{lemma}[Lower bound for recursive concatenation of strictly monotonously increasing functions]
	\label{lemma:self-lower-bound-strictly-monotonous-functions}
	Let $f:\mathbb{N}^{k}\to\mathbb{N}$ be an arbitrary $k$-ary function that is strictly monotonously increasing with $k\geq 1$.
	Let $f^{(m)}$ denote the $m$-wise recursive concatenation of $f$ as in Definition \ref{def:recursive-concatenation}. 
	% NOTE: It must hold that $k\geq 1$ for strictly monotonously increasing functions.
	
	It holds that $f^{(m)}(n)\geq k^m\cdot n$ for all $n,m\in\mathbb{N}$.
\end{lemma}
\begin{proof}
	Fix an arbitrary, strictly monotonously increasing function $f$ of arity $k$.
	First of all, we show that $f^{-}(n)\geq k\cdot n $ for all $n\in\mathbb{N}$.
	
	Exhaustive exploitation of strict monotonicity yields
	\begin{align}
		f^{-}(n)=f(\underbrace{n,\dots,n}_{k \text{ times}})&\geq f(n-1,n,\dots,n)+1\\
		&\overset{\dots}{\geq} f(0,n,\dots,n)+n \\
		&\overset{\dots}{\geq} \underbrace{f(0,0,\dots,0)}_{\geq 0}+k\cdot n \geq k\cdot n.
		\label{eq:self-lower-bound-strictly-monotonous-functions-inequality-depth1}
	\end{align} 
	
	Now, we prove the general lower bound for $f^{(m)}(n)$ by induction over the number of recursive concatenations $m\in\mathbb{N}$.
	
	The base case $m=0$ holds by definition since $f^{(0)}(n)=n=k^0\cdot n$.
	
	For the induction hypothesis (IH), assume that there is some $m\in\mathbb{N}$ such that $f^{(m)}(n)\geq k^m\cdot n$ holds for all $n\in\mathbb{N}$.
	By the strict monotonicity of $f$ and Equation \ref{eq:self-lower-bound-strictly-monotonous-functions-inequality-depth1}, we obtain
	\begin{align}
		\label{eq:self-lower-bound-strictly-monotonous-functions-inequality-induction-step}
		f^{(m+1)}(n)&=f^{-}(f^{(m)}(n)) \overset{\text{(IH)},(\ref{eq:self-lower-bound-strictly-monotonous-functions-inequality-depth1})}{\geq} k\cdot \left(k^m\cdot n \right) = k^{m+1} \cdot n. 
	\end{align}
	
	Consequently, the overall result follows by the induction principle.
\end{proof}

\section{Expressive limits of non-recursive models}
\label{sec:expressive-limits-non-recursive-models}
For the subsequent theorems, we zero in on the recursive concatenation of a specific function, namely the sum over the function set $\tau$, defined as $f_{\tau}:=\left(\sum_{i=1}^{j}f_j\right)$.
% If the building block functions $f_i$ satisfy certain assumptions, taking the sum over $\tau$ will ensure that no non-recursive function in $\mathcal{F}_{\tau}$ can keep up with the growth of the recursive concatenation $f_{\tau}^{(n)}(n)$ for increasing $n\in\mathbb{N}$.

To illustrate why it is necessary to take the sum over \textit{all} functions in $\tau$, consider the addition function $f_{+}:\mathbb{N}\times \mathbb{N}\to\mathbb{N}$ as an ubiquitous example.
If the function set $\tau=\{f_{+},f\}$ fortuitously happened to include another function $f$ that is just precisely defined as the recursive concatenation of the addition function $f(m,n):=f_{+}^{(m)}(n)=2^{m}\cdot n$, the recursive concatenation of the addition function alone would be easily expressible by non-recursive models on $\tau$ by merely applying $f$ once to the input.
But if we instead take the recursive concatenation of their sum $f_{\tau}(n)=2n+2^{n} n = (2^{n}+2)\cdot n$, we can prove that there is no non-recursive function $f\in\mathcal{F}_{\tau}$ that can keep up with its super-exponential growth, rendering this recursive concatenation inexpressible in $\mathcal{F}_\tau$.

In general however, this inexpressivity statement is hard to prove if we do not make some assumptions on the building block functions $f_i\in\tau$.
At the same time, we do not want to restrict $\tau$ too much to maintain the pertinence of this result for machine learning models in practice.
It turns out that for any function sets $\tau$ where each function is either strictly monotonously increasing or bounded, we achieve a sound trade-off between these two conflicting objectives.
On the one hand, the formal proofs render quite simple while on the other hand, the representation classes of many machine learning models still fall into this restricted category of function sets $\mathcal{F}_{\tau}$.
Apart from the $ReLU$ activation function, most common activation functions such as $ELU$, $\sigma$, or $\tanh$ are strictly monotonously increasing. At the same time, ubiquitous arithmetic operations like addition or multiplication are strictly monotonously increasing, too.
Over and above, learned parameters of a neural network can be represented by constants, which obviously constitute bounded functions.
Finally, logical functions are by definition bounded by $1$, and in particular allow to indirectly realise the $ReLU$ activation function and other operations that require case distinctions.

For that reason, we restrict the function sets $\tau$ to strictly monotonously increasing and bounded functions.
As it renders useful in the later theorems, we now show that for any such $\tau$, the recursive concatenation of $f_{\tau}$ is lower bounded by the recursive concatenation of each strictly monotonously increasing function $f_k\in\tau$.

\begin{lemma}[Lower bound for recursive concatenation of function set sum]
	\label{lemma:max-bound-recursive-concatenation-sum}
	Given an arbitrary function set $\tau=\{f_1,\dots,f_j\},f_i:\mathbb{N}\to\mathbb{N}$.
	
	For any monotonously increasing function $f_k\in\tau$ and any $m,n\in\mathbb{N}$, it holds that
	$f_{\tau}^{(m)}(n)\geq f_k^{(m)}(n)$.
\end{lemma}
\begin{proof}
	Given all as above.
	We show the statement by induction over $m\in\mathbb{N}$.
	
	For $m=0$, the result follows from the definition
	\begin{align*}
		f_{\tau}^{(0)}(n)=\sum_{i=1}^{j} f_{i}^{(0)}(n)=\sum_{i=1}^{j}n\geq n=f_{k}^{(0)}(n).
	\end{align*}
	for all $1\leq k\leq j$ and $n\in\mathbb{N}$.
	
	Now, assume that for some $m\in\mathbb{N}$, the statement $f_{\tau}^{(m)}(n)\geq f_i^{(m)}(n)$ holds for all monotonously increasing functions $f_i\in\tau$ and all $n\in\mathbb{N}$.
	
	For $m+1$, any monotonously increasing $f_k \in \tau$ satisfies
	\begin{align}
		\label{eq:lemma:max-bound-recursive-concatenation-sum-inequality}
		f_{\tau}^{(m+1)}(n) = f_{\tau}^{-}(f_{\tau}^{(m)}(n)) = \sum_{i=1}^{j}f_i^{-}(f_{\tau}^{(m)}(n))
		&\geq f_k^{-}(f_{\tau}^{(m)}(n))\\
		&\overset{\text{(IH)}}{\geq} f_k^{-}( f_k^{(m)}(n)) = f_k^{(m+1)}(n),
	\end{align}
	where the first inequality draws upon the non-negativity of $\mathbb{N}$.
	Therefore, the overall statement for all $m\in\mathbb{N}$ follows by induction.	
	
\end{proof}

In the same spirit as in Definition \ref{def:recursive-concatenation}, we merge the two variables $m,n$ of the recursive concatenation into one single input $n$ for convenience, and refer to the resulting function as the \textit{recursive completion} of $f$.

\begin{definition}[Recursive completion]
	\label{def:recursive-completion}
	Let $f:\mathbb{N}^k\to\mathbb{N}$ be an arbitrary function with arity $k\geq 1$. 
	
	We define the recursive completion of $f$, denoted by $f^{\lozenge}:\mathbb{N}\to\mathbb{N}$, as
	\begin{align}
		f^{\lozenge}(n) &= f^{(n)}(n), \quad n\in\mathbb{N},
	\end{align}
	where $f^{(n)}$ denotes the $n$-wise recursive concatenation of $f$ as in Definition \ref{def:recursive-concatenation}.
\end{definition}
The following theorems will be proved for this special case of recursive concatenations.
For an arbitrary function set $\tau$ that only comprises strictly monotonously increasing and bounded functions, we are going to show that $f_\tau^{\lozenge}$ --- referred to as the \textit{recursive completion over $\tau$} --- is not expressible by non-recursive functions over $\tau$, briefly $f_\tau^{\lozenge} \notin \mathcal{F}_{\tau}$.
This implies that any non-recursive model on $\tau$ is not able to express this function.

\begin{theorem}[Recursive completion over $\tau$ is not non-recursively expressible]
	\label{theorem:recursive-completion-not-non-recursive-expressible}
	Fix an arbitrary function set $\tau=\{f_1,\dots,f_j\},f_i:\mathbb{N}\to\mathbb{N}$, where each $f_i$ is strictly monotonously increasing or bounded. 
	Assume that there is at least one $f_i\in\tau$ that is strictly monotonously increasing and has arity $\operatorname{ar}(f_i)>1$, e.g. the addition function.
	For any non-recursive function $f\in\mathcal{F}_{\tau}$, there is an $n_0\in\mathbb{N}$ such that for all $n\geq n_0$, $f(n)\neq f_{\tau}^{\lozenge}(n)$.
\end{theorem}
\begin{proof}
	Fix an arbitrary function set $\tau=\{f_1,\dots,f_j\}$, where each $f_i$ is strictly monotonously increasing or bounded and at least one $f_i\in\tau$ is strictly monotonously increasing with arity $\operatorname{ar}(f_i)>1$. 
	Define $f_\tau(n):=\sum_{i=1}^{j}f_i(n)$.
	
	As it facilitates the induction step, we prove a stronger statement by induction over the depth of non-recursive functions $\operatorname{dep}(f)\in\mathbb{N}$.
	Namely, we are going to prove that for all non-recursive functions $f\in\mathcal{F}_{\tau}$, there is an $n_0\in\mathbb{N}$ such that $f(n+a)<f_{\tau}^{(n)}(n+a)$ for all $n\geq n_0$ and all offsets $a\in\mathbb{N}$.
	
	Beginning with the base case, let $f\in\mathcal{F}_{\tau}$ be arbitrary with $\operatorname{dep}(f)=0$.
	Therefore, $f$ must be equivalent to the identify function, thus $f(n)=n$ for all $n\in\mathbb{N}$.
	
	Choose $n_0=1$ and let $n\geq n_0$ and $a\in\mathbb{N}$ be arbitrary.
	
	Since $k=\operatorname{ar}(f_m)>1$ for at least one strictly monotonously increasing $f_m\in\tau$, Lemma \ref{lemma:self-lower-bound-strictly-monotonous-functions} guarantees
	\begin{align}
		\label{eq:theorem:recursive-completion-not-non-recursive-expressible-self-lower-bound}
		f_m^{(n)}(n+a) \geq k^n\cdot (n+a) \overset{k>1}{>} n+a.
	\end{align}
	
	% NOTE: if we had negative numbers, this overall statement does not hold.
	% - The sum could be smaller than some monotonously increasing function.
	% - -> Adjust Lemma \ref{lemma:self-lower-bound-strictly-monotonous-functions} to still include the bounded functions as additive terms. We can only eradicate the other strictly monotonously terms.
	
	% To let this proof transfer more directly to countably infinite sets with negative numbers like $\mathbb{Q}$, we do not avail of the fortuitous fact $f_m^{-}(0)\geq 0$ for having chosen $\mathbb{N}$ but conduct a more general argument that is agnostic of $f_m^{-}(0)$.
	
	Since $f_m$ is monotonously increasing, we conclude by means of Lemma \ref{lemma:max-bound-recursive-concatenation-sum} that
	\begin{equation}
		\label{eq:theorem:recursive-completion-not-non-recursive-expressible-inequality}
		f_{\tau}^{(n)}(n+a)\overset{\ref{lemma:max-bound-recursive-concatenation-sum}}{\geq} f_m^{(n)}(n+a) \overset{(\ref{eq:theorem:recursive-completion-not-non-recursive-expressible-self-lower-bound})}{>} n+a = f(n+a).
	\end{equation}
	
	Proceeding with the induction hypothesis (IH), assume that there is some $p\in\mathbb{N}$ such that for every non-recursive function $f\in\mathcal{F}_{\tau}$ with $\operatorname{dep}(f)\leq p$, there exists an $n_0\in\mathbb{N}$ such that $f_\tau^{(n)}(n+a)>f(n+a)$ for all $n\geq n_0$ and $a\in\mathbb{N}$.
	
	Let $a\in\mathbb{N}$ be arbitrary in the following.
	Consider an arbitrary non-recursive function $f\in\mathcal{F}_{\tau}$ with $\operatorname{dep}(f)=p+1$.
	
	As by Definition \ref{def:non-recursive-functions}, we have $f(n)=f_m(g_1(n),\dots,g_k(n))$ for some $f_m\in\tau$ with arity $k\in\mathbb{N}$ and non-recursive functions $g_i\in \mathcal{F}_{\tau}$ with depth $\operatorname{dep}(g_i)\leq p$.
	
	To begin with, we consider the case where $f_m$ is a bounded function. Denote by $c_m$ the bound of $f_m$.
	By Equation \ref{eq:theorem:recursive-completion-not-non-recursive-expressible-inequality}, we have  
	\begin{equation}
		\label{eq:theorem:recursive-completion-not-non-recursive-expressible-induction-step-bounded}
		f_\tau^{(n)}(n+a)>n+a\geq c_m\geq f_m(g_1(n+a),\dots,g_k(n+a))=f(n+a)
	\end{equation}
	for any $n\geq c_m$.
	
	Otherwise, consider the case where $f_m$ is strictly monotonously increasing.
	By the induction hypothesis, for every non-recursive function $g_i$, there is an $n_0(g_i)$ such that $f_\tau^{(n)}(n+a)>g_i(n+a)$ for all $n\geq n_0(g_i)$.
	Since there are only $k$ non-recursive functions $g_i$, we directly take $n_0$ as the maximum $n_0 = \max_{1\leq i\leq k}n_0(g_i)$.
	
	Now, let $n\geq n_0$ be arbitrary. 
	Subsequently, we assume that $a\geq 1$, and substitute $b=a-1$ and $n'=n+1$.
	By the monotonicity of $f_m$, we conclude
	\begin{align}
		\label{eq:theorem:recursive-completion-not-non-recursive-expressible-induction-step}
		f(n'+b)=f(n+a)&=f_m(g_1(n+a),\dots,g_k(n+a))\\
		&\overset{(IH)}{<}f_m(f_\tau^{(n)}(n+a),\dots,f_\tau^{(n)}(n+a))=f_m^{-}(f_\tau^{(n)}(n+a))\\
		&\leq f_\tau^{-}(f_\tau^{(n)}(n+a))=f_{\tau}^{(n+1)}(n+a)=f_{\tau}^{(n')}(n'+b).
	\end{align}
	
	Since $a\in\mathbb{N}_{\geq 1}$ was chosen arbitrarily, the above argument holds for all $b\in\mathbb{N}$ and all $n'\geq n_0+1=:n_0(f)$. 
	To conclude with, we have shown that for any non-recursive function $f\in \mathcal{F}_{\tau}$ with $\operatorname{dep}(f)=p+1$, there exists an $n_0(f)\in\mathbb{N}$ such that $f(n+a)<f_\tau^{(n)}(n+a)$ for all $n\geq n_0(f),a\in\mathbb{N}$.
	
	Consequently, the overall result for non-recursive functions of arbitrary depth draws upon the induction principle.
	
	As a special case of the above result, we obtain that for any non-recursive function $f\in \mathcal{F}_{\tau}$, there is an $n_0\in\mathbb{N}$ such that for all $n\geq n_0$, $f(n)<f_{\tau}^{\lozenge}(n)$.
	
\end{proof}

Before we continue, let us make a few comments on the proof of the theorem.
In the induction step for non-recursive functions $f$ of depth $p+1$, the threshold $n_0$ only increased by $1$ when the outermost function $f_m$ was strictly monotonously increasing. 
Otherwise, if $f_m$ was a bounded function, the threshold $n_0$ could just be taken as the bound $c_m$ of $f_m$, irrespective of the depth of $f$.
Therefore, for non-recursive functions $f$ of sufficiently large depth $p$, the threshold $n_0$ can be upper bounded by a term $a\cdot p$, where $a\to 1$ for $p\to\infty$.  

Figure \ref{fig:composition-tree-recursive-completion-vs-fixed-term} visualizes the eventual dominance of $f_{\tau}^{\lozenge}$ over non-recursive functions $f$.
$f_{\tau}$ covers every possible \textit{composition path} that the non-recursive functions in $\tau$ could possibly choose. Eventually, $n$ will exceed both the depth of $f$ and the largest constant in $f$. By the the strict monotonicity of the involved functions, $f_{\tau}^{\lozenge}$ will thus excel $f$.

An extension of this result to $\mathbb{Z}$ allows to apply this result directly to neural networks which usually operate over signed floating point numbers of finite precision.
The adjustments we need to make are minimal.
On the one hand, the inequality in Equation (\ref{eq:lemma:max-bound-recursive-concatenation-sum-inequality}) in the proof of the lower bound of the sum $f_\tau:=\sum_{i=1}^{j}f_i$ in Lemma \ref{lemma:max-bound-recursive-concatenation-sum} relied on the non-negativity of each $f_i$.
To generalize this statement to functions over $\mathbb{Z}$, we simply wrap each function $f_i$ by the \textit{magnitude} function before summing them up.
Juxtaposing this adjusted construction $f_{\tau}$ to non-recursive functions over $\tau$ is still perfectly fair, since both the addition function and the magnitude function will still fall within the scope of functions that still satisfy our assumptions on $\tau$.

On the other hand, the assumptions on the functions in $\tau$ are weakened a little to still express a broad range of functions that are used to design neural networks and other models in practice.
While the definition of boundedness directly carries over to negative numbers, the condition of strict monotonicity is slightly alleviated.

Arithmetic operations such as multiplication and addition, logical expressions, constants, and activation functions that are usually employed in practice such as ReLU, ELU, $\tanh$, or sigmoid still fall within the scope of these assumptions.

Although the proofs require only slight changes, they yet affect the ease of exposition.
For this reason, the argument was presented here only for functions over $\mathbb{N}$, while the more general result for functions over $\mathbb{Z}$ is laid out in Appendix \ref{app:extension-inexpressivity-integer}.

\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[every text node part/.style={align=center}, font=\normalsize]
			%% UPPER LAYER
			\node[] (ftau0) at (0,0) {$f_{\tau}^{\lozenge}(n)=\Sigma_{i=1}^{2}f_i^{\lozenge}(n)$};
		
				\node[below = 20pt of ftau0, xshift=-50pt] (f10){$f_1$};
				\node[below = 20pt of ftau0, xshift=50pt] (f20){$f_2(\;\cdot\;,\;\cdot\;,\;\cdot\;)$};
				\node[below = 0pt of ftau0, xshift=5pt] (+0){$+$};
				
				\draw[-] ([xshift=0pt]ftau0.south) -- (f10.north);
				\draw[-,thick,color=rwth-carmine] ([xshift=10pt]ftau0.south) -- (f20.north);
				
				%%% MIDDLE LAYER
				\node[below = 20pt of f20,xshift=-45pt] (ftau10){$f_{\tau}^{(n-1)}$};
				\node[below = 20pt of f20,xshift=5pt] (ftau11){$f_{\tau}^{(n-1)}$};
				\node[below = 20pt of f20,xshift=55pt] (ftau12){$f_{\tau}^{(n-1)}$};
				
				\draw[-,color=rwth-carmine] ([xshift=-10pt]f20.south) -- (ftau10.north);
				\draw[-,color=rwth-carmine] ([xshift=5pt]f20.south) -- (ftau11.north);
				\draw[-,color=rwth-carmine] ([xshift=20pt]f20.south) -- (ftau12.north);
				
				%% LOWER LAYER
				\node[below=20pt of ftau10,xshift=-20pt] (f1-ul) {$f_1$};
				\node[below=20pt of ftau10,xshift=5pt] (f2-ul) {$f_2$};
				\node[below = 5pt of ftau10, xshift=-7pt] (+10){\small $+$};
				\draw[-,thick,color=rwth-carmine] ([xshift=-10pt]ftau10.south) -- (f1-ul.north);
				\draw[-] ([xshift=-5pt]ftau10.south) -- (f2-ul.north);
				
				\node[below=20pt of ftau11,xshift=-20pt] (f1-um) {$f_1$};
				\node[below=20pt of ftau11,xshift=5pt] (f2-um) {$f_2$};
				\node[below = 5pt of ftau11, xshift=-7pt] (+11){\small $+$};
				\draw[-] ([xshift=-10pt]ftau11.south) -- (f1-um.north);
				\draw[-,thick,color=rwth-carmine] ([xshift=-5pt]ftau11.south) -- (f2-um.north);
				
				\node[below=20pt of ftau12,xshift=-20pt] (f1-ur) {$f_1$};
				\node[below=20pt of ftau12,xshift=5pt] (f2-ur) {$f_2$};
				\node[below = 5pt of ftau12, xshift=-7pt] (+12){\small $+$};
				\draw[-] ([xshift=-10pt]ftau12.south) -- (f1-ur.north);
				\draw[-,thick,color=rwth-carmine] ([xshift=-5pt]ftau12.south) -- (f2-ur.north);
				
				%% BOTTOM LAYER
				\node[below=20pt of f2-ul] (nl) {$n$};
				\draw[loosely dotted] ([xshift=0pt]f2-ul.south) -- (nl.north);
				\node[below=20pt of f2-um] (nm) {$n$};
				\draw[loosely dotted] ([xshift=0pt]f2-um.south) -- (nm.north);
				\node[below=20pt of f2-ur] (nr) {$n$};
				\draw[loosely dotted] ([xshift=0pt]f2-ur.south) -- (nr.north);
			
			
			%%%%%% SMALL FUNCTION DESCRIPTION %%%%%%%%%%%%%%
		
				\node[right=30pt of ftau0, color=gray] (descr) {{\normalsize $\substack{f_1 \text{ const.}, \\f_2 \text{ str. mon. incr. 3-ary}}$}};
				\draw[dashed, color=gray] (ftau0.east) -- (descr.west);
			
			
			%%%%%% RIGHT SIDE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\node[right=150pt of ftau0] (rftau0) {$f=f_2(f_1,f_2(\dots),f_2(\dots))$};
		
				\node[below = 20pt of rftau0, xshift=-100pt, color=lightgray,opacity=0.5] (rf10){$f_1$};
				\node[below = 20pt of rftau0, xshift=0pt] (rf20){$f_2(\;\cdot\;,\;\cdot\;,\;\cdot\;)$};
				
				\draw[-, color=lightgray,opacity=0.5] ([xshift=-55pt]rftau0.south) -- (rf10.north);
				\draw[-, thick,color=rwth-carmine] ([xshift=-45pt]rftau0.south) -- (rf20.north);
				
				%%% MIDDLE LAYER
				\node[below = 33pt of rf20,xshift=-50pt] (rftau10){};
				\node[below = 33pt of rf20,xshift=0pt] (rftau11){};
				\node[below = 33pt of rf20,xshift=50pt] (rftau12){};
				
				%% LOWER LAYER
				\node[below=20pt of rftau10,xshift=-10pt] (rf1-ul) {$f_1$};
				\node[below=20pt of rftau10,xshift=10pt, color=lightgray,opacity=0.5] (rf2-ul) {$f_2$};
				\draw[-, thick,color=rwth-carmine] ([xshift=-15pt]rf20.south) -- (rf1-ul.north);
				\draw[-, color=lightgray,opacity=0.5] ([xshift=-10pt]rf20.south) -- (rf2-ul.north);
				
				\node[below=20pt of rftau11,xshift=-5pt, color=lightgray,opacity=0.5] (rf1-um) {$f_1$};
				\node[below=20pt of rftau11,xshift=15pt] (rf2-um) {$f_2$};
				\draw[-, color=lightgray,opacity=0.5] ([xshift=2pt]rf20.south) -- (rf1-um.north);
				\draw[-, thick,color=rwth-carmine] ([xshift=7pt]rf20.south) -- (rf2-um.north);
				
				\node[below=20pt of rftau12,xshift=0pt, color=lightgray,opacity=0.5] (rf1-ur) {$f_1$};
				\node[below=20pt of rftau12,xshift=20pt] (rf2-ur) {$f_2$};
				\draw[-, color=lightgray,opacity=0.5] ([xshift=20pt]rf20.south) -- (rf1-ur.north);
				\draw[-, thick,color=rwth-carmine] ([xshift=25pt]rf20.south) -- (rf2-ur.north);
			
				\node[below=20pt of rf2-ul, color=lightgray,opacity=0.5] (rnl) {$f_1/n$};
				\draw[loosely dotted, color=lightgray,opacity=0.5] ([xshift=0pt]rf2-ul.south) -- (rnl.north);
				\node[below=20pt of rf2-um] (rnm) {$f_1/n$};
				\draw[loosely dotted] ([xshift=0pt]rf2-um.south) -- (rnm.north);
				\node[below=20pt of rf2-ur] (rnr) {$f_1/n$};
				\draw[loosely dotted] ([xshift=0pt]rf2-ur.south) -- (rnr.north);
				
			
			%%%%%%%%%%% DOMINANCE ARROW %%%%%%%%%%%%%
			
				\draw[-stealth, dashed, color=rwth-carmine] ([xshift=-25pt,yshift=-20pt]descr.east) -- ++ (0pt,-120pt) node[midway,right] {\small $\operatorname{dep}(f)\in o(n)$};
				\draw[-stealth, dashed, color=rwth-carmine] (nr.east) --++(90pt,0pt) node[midway,above]{\small $f_1\in o(n)$};
			
		\end{tikzpicture}
	\end{center}
	\caption[Dominance of the recursive completion over non-recursive functions.]{Term tree juxtaposing the recursive completion $f_{\tau}^{\lozenge}$ and an exemplary non-recursive function $f\in\mathcal{F}_{\tau}$ of fixed depth. Eventually, $n$ will excel both the depth of $f$ and the largest constant in $f$, which ensures that $f_{\tau}^{\lozenge}(n)>f(n)$.}
	\label{fig:composition-tree-recursive-completion-vs-fixed-term}
\end{figure}

This inexpressivity result is preserved if we scale the input $n$ of $f_\tau^{\lozenge}$ by an arbitrary positive parameter $a\in\mathbb{N}$. 
The proof is analogous to Theorem \ref{theorem:recursive-completion-not-non-recursive-expressible} and therefore deferred to Appendix \ref{app:scaled-recursive-completion-inexpressible}.
\begin{corollary}[Scaled Recursive Completion is not non-recursively expressible]
	\label{cor:scaled-recursive-completion-not-non-recursive-expressible-placeholder}
	Let $\tau$ be as in Theorem \ref{theorem:recursive-completion-not-non-recursive-expressible}.
	
	For any $a\in\mathbb{N}$, define the scaled recursive completion over $\tau$ as
	$\left(f_{\tau}\right)_{a}^{\lozenge}(n):=f_{\tau}^{(n)}(a\cdot n)$.
	
	Then, $\left(f_{\tau}\right)_{a}^{\lozenge}\notin \mathcal{F}_{\tau}$ for any $a \geq 1$.
	In particular, let $f\in\mathcal{F}_{\tau}$ be an arbitrary non-recursive function over $\tau$.
	Then, for the same $n_0(f)$ as in Theorem \ref{theorem:recursive-completion-not-non-recursive-expressible}, we have that for all $a\in\mathbb{N}$ with $a\geq 1$ and $n\geq n_0$, $f(n)\neq \left(f_{\tau}\right)_{a}^{\lozenge}(n)$ for all $n\geq n_0(f)$.
\end{corollary}

On the one hand, we proved that the recursive completion over $\tau$ is inexpressible by $\mathcal{F}_{\tau}$ under feasible assumptions.
On the other hand, it remains to argue that $f_\tau^{\lozenge}$ maintains a low Kolmogorov complexity when the description of the underlying function set $\tau$ is already given.
In other words, a program to compute $f_{\tau}^{\lozenge}$ is simple to express with the functions $f_i\in\tau$.
It turns out that there is a constant $c\in\mathbb{N}$ across all finite sets of partial computable functions $\tau$ that bounds the description length of such a program given a description of $\tau$.

\section{The uniform simplicity of recursive completion}
\label{sec:uniform-simplicity-recursive-completion}
We cannot demand from our models to be able to learn functions that are arbitrarily complex to describe with the underlying function set $\tau$.

For the recursive completion $f_{\tau}^{\lozenge}$ however, we will show that given a description of $\tau$, the Kolmogorov complexity of $f_{\tau}^{\lozenge}$ is upper bounded by a constant that is independent of the function set $\tau$.

To establish this statement, we will construct a Turing Machine $\mathcal{T}_{\lozenge}$ that first receives a description of the functions $f_i\in\tau$ and the scale $a\in\mathbb{N}$ as parameters, and then computes $\left(f_{\tau}\right)_a^{\lozenge}(n)$ for any $n\in\mathbb{N}$ that is appended as input. 

If $f_i \in\tau$ is partial computable, there exists a Turing Machine $\mathcal{T}_i$ that computes $f_i$.
Providing the encoding $\operatorname{enc}(\mathcal{T}_i)$ of such a Turing Machine and the arity of $f_i$ will constitute a sufficient description of $f_i$.
The encoding $\operatorname{enc}(\tau)$ can therefore be thought of as a self-delimiting encoding of the \textit{sequence} $\bigl[\operatorname{enc}(\mathcal{T}_1),\operatorname{ar}(f_1),\dots,\operatorname{enc}(\mathcal{T}_j),\operatorname{ar}(f_j)\bigr]$.
In the spirit of Equation \ref{eq:self-delimiting-encoding}, the self-delimiting encoding of a sequence $[z_1,\dots,z_n]$ of binary strings $z_i$ is defined as
\begin{equation}
	\label{eq:self-delimiting-enc-sequence}
	[z_1,\dots,z_n]:=[z_1,\varepsilon][z_2,\varepsilon]\dots[z_{n-1},\varepsilon]z_n\in\{0,1\}^{*},
\end{equation}
where $\varepsilon\in\{0,1\}^{*}$ again is the empty string as introduced in Definition \ref{def:natural-numbers-binary-strings-encoding}.

Given such an encoding, $\mathcal{T}_{\lozenge}$ can then simulate the Turing Machines $\mathcal{T}_i$ just as the universal Turing Machine $U$ from Equation \ref{eq:universal-turing-machine}, and sum up their outputs to obtain $f_{\tau}$.

Of course, we have to make the assumption that every function $f_i\in\tau$ is partial computable.
But any function that can be computed by a machine must necessarily be partial computable according to the Church-Turing thesis.
% Explain the Church-Turing thesis?
Therefore, this assumption is perfectly feasible in practice.  

%% CONDITIONAL KOLMOGOROV COMPLEXITY INSTEAD OF UNCONDITIONAL
Using the \textit{unconditional} Kolmogorov complexity $K(\cdot)$ to quantify how the descriptive complexity $K\left(f_{\tau}^{\lozenge}\right)$ evolves across different $\tau$ would however be deceptive for our argument, since it interfuses the expressive power of the \textit{model structure} with the expressive power of the underlying building block function set $\tau$.
The non-recursivity of models is however only property of the former, while the latter can be thought of as the prior knowledge the model can assume to express patterns.
Therefore, we are instead going to use the \textit{conditional} Kolmogorov complexity $K\bigl(\left(f_{\tau}\right)_{a}^{\lozenge}\mid \operatorname{enc}(\tau)\bigr)$.
In this way, our argument substantiates that such non-recursive \textit{structure} hinders models from expressing functions that yet would have been simple to describe with the underlying function set $\tau$.

% Accordingly, the following theorem yields a complexity upper bound in terms of $K\bigl(\left(f_{\tau}\right)_{a}^{\lozenge}\mid \operatorname{enc}(\tau)\bigr)$ and not merely in terms of $K\bigl(\left(f_{\tau}\right)_{a}^{\lozenge}\bigr)$.

\begin{theorem}[Uniform Kolmogorov Complexity Bound for Recursive Completion]
	\label{theorem:recursive-completion-kolmogorov-complexity}
	There exists a constant $c\in\mathbb{N}$ such that the following holds:
	
	Given an arbitrary finite set of partial computable functions $\tau=\{f_1,\dots,f_j\}$. 
	Let $\mathcal{T}_{i}$ be a Turing Machine that computes $f_i$.
	Denote by $\operatorname{enc}(\tau)$ the self-delimiting encoding of the sequence $\bigl[\operatorname{enc}(\mathcal{T}_1),\operatorname{ar}(f_1),\dots,\operatorname{enc}(\mathcal{T}_j),\operatorname{ar}(f_j)\bigr]$ as in Equation \ref{eq:self-delimiting-enc-sequence}.
	
	Then, $K(\left(f_{\tau}\right)_{a}^{\lozenge}\mid \operatorname{enc}(\tau))\leq c+2\cdot \log_2(a)$.
	
\end{theorem}
\begin{proof}
	Given $\tau$ and $\operatorname{enc}(\tau)$ as above.
	
	We define a Turing Machine $\mathcal{T}_{\lozenge}$ that computes the scaled recursive completion in the following way:
	$\mathcal{T}_{\lozenge}$ expects its input in the self-delimiting format $\bigl[\operatorname{enc}(\tau),[x_a,x_n]\bigr]$, where $x_a$ and $x_n$ are the binary encodings of the parameter $a$ and the argument $n$.
	
	Initially, $\mathcal{T}_{\lozenge}$ verifies the syntactical correctness of its input.
	In particular, it segments the input string according to the delimiters and checks whether the string segments for $n$,$a$, or the arities $\operatorname{ar}(f_i)$ truly encode an integer in the correct format.
	Moreover, it checks whether the string segments for $\mathcal{T}_i$ actually encode valid G\"odel numbers.
	If any of the above checks fail, $\mathcal{T}_{\lozenge}$ halts.
	
	Besides the input tape and the output tape, $\mathcal{T}_{\lozenge}$ also uses three auxiliary tapes, the \textit{counting tape}, the \textit{computation tape}, and the \textit{accumulation tape}.
	First, $\mathcal{T}_{\lozenge}$ writes the integer input $n$ both on the counting tape and the output tape.
	On the output tape, it further multiplies this integer by $a$.
	
	Now, $\mathcal{T}_{\lozenge}$ repeats the ensuing procedure until the counting tape is eradicated blank. 
	\begin{enumerate}
		\item Clean the computation tape and the accumulation tape from any non-blank symbols. Write a $0$ on the accumulation tape.
		\item Interpret the binary sequence $x$ on the output tape as a natural number $x_\ell,\ell\in\mathbb{N}$.
		\item From $i=1$ to $i=j$, fetch the G\"odel number of $\mathcal{T}_i$ and its input arity $k:=\operatorname{ar}(f_i)$ from the input tape. Invoke $\mathcal{T}_i$ on the $k$-wise self-delimited concatenation of $x_\ell$ and add its output to the encoded integer on the accumulation tape.
		\item After having added the output of all $j$ Turing Machines, copy the encoded string from the accumulation tape to the output tape.
		\item Subtract the integer encoded on the counting tape by $1$.
	\end{enumerate}
	After this iterative procedure terminates, $\mathcal{T}_{\lozenge}$ halts immediately.
	
	In each iteration, if $\ell$ is the integer encoded on the output tape by $x_\ell$, the accumulation tape with contain the value $f_{\tau}^{-}(\ell)$.
	Since $\ell=a\cdot n$ at the very beginning, the string $x$ written to the output tape after the $m$th iteration will encode $f_{\tau}^{(m)}(a\cdot n)$.
	As the initial argument $n$ is written on the counting tape too, exactly $n$ iterations will be performed.
	Therefore, the final output $x$ encodes $f_{\tau}^{(n)}(a\cdot n)=\left(f_{\tau}\right)_a^{\lozenge}(n)$. 
	Denote by $f_{\lozenge}=f_{\mathcal{T}_{\lozenge}}$ the partial computable function that $\mathcal{T}_{\lozenge}$ computes. Then, $f_{\lozenge}\bigl(\bigl[\operatorname{enc}(\tau),[x_a,x_n]\bigr]\bigr)=\left(f_{\tau}\right)_a^{\lozenge}(n)$ for all $a,n\in\mathbb{N}$.
	
	To execute $\mathcal{T}_{\lozenge}$ on the universal Turing Machine $U$,
	we need to provide $\operatorname{enc}(\mathcal{T}_{\lozenge})\bigl[\operatorname{enc}(\tau),[x_a,x_n]\bigr]$
	Let $c_0$ denote the length of the G\"odel number $\operatorname{enc}(\mathcal{T}_{\lozenge})$.
	Comparing the self-delimiting encoding $[x_a,x_n]=[x_a,\varepsilon]x_n=yx_n$, we find that $y=[x_a,\varepsilon]$ comprises $2\cdot l(x_a)+1 = 2\cdot \lceil \log_2(a)\rceil +1 \leq 3 + 2\cdot \log_2(a)$ bits.
	
	With $c:=c_0+3$, we therefore finally obtain
	\begin{align}
		K(\left(f_{\tau}\right)_{a}^{\lozenge}\mid \operatorname{enc}(\tau))
		&= \min_{p,p'\in\{0,1\}^{*}}\{l(p)+l(p')\mid U\bigl(p[\operatorname{enc}(\tau),p'x_n]\bigr)=\left(f_{\tau}\right)_{a}^{\lozenge}(x_n) \text{ for all } n\in\mathbb{N}\}\\
		& \leq l(\operatorname{enc}(\mathcal{T}_{\lozenge})) + l([x_a,\varepsilon])\\
		& = c + 2\cdot \log_2(a).
	\end{align}
\end{proof}
Figure \ref{fig:uniform-tm-recursive-completion} sketches the interplay of the different tapes that $\mathcal{T}_{\lozenge}$ orchestrates to compute $f_{\tau}^{\lozenge}$.
\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[every text node part/.style={align=center}, font=\small]
			\node[] (input) at (140pt,-10pt) {input tape\\ 
				\begin{tabular}{c|c|c|c|c|c|c|c}
					\hline
					$\dots$
					& $\operatorname{enc}(\mathcal{T}_1)$ 
					& $\operatorname{ar}(f_1)$
					& $\dots$
					& $\operatorname{enc}(\mathcal{T}_j)$ 
					& $\operatorname{ar}(f_j)$
					& $n$ 
					& $\dots$ \\\hline
				\end{tabular}
			};
			
			%% TURING MACHINE SIGNIFIER
			\node[above=-10pt of input.north west,xshift=-10pt] (tm) {$\mathcal{T}_{\lozenge}$:};
			
			%% COUNTING
			\node[right= 30pt of input.south east, yshift=-20pt] (counting){loop tape \\
				\begin{tabular}{c|c|c}
					\hline
					$\dots$
					& $k$
					& $\dots$ \\\hline
				\end{tabular}
			};
			
			%% OUTPUT
			\node[below= 30pt of counting.south] (output){output tape \\
				\begin{tabular}{c|c|c}
					\hline
					$\dots$
					& $x$
					& $\dots$ \\\hline
				\end{tabular}
			};
			
			%% COMPUTATION
			\node[below= 30pt of input.south west, xshift=60pt] (computation){computation tape \\
				\begin{tabular}{c|c|c}
					\hline
					$\dots$
					& \footnotesize simulate $\mathcal{T}_i$ on $x$ 
					& $\dots$ \\\hline
				\end{tabular}
			};
			
			%% ACCUMULATION
			\node[below= 30pt of computation.south] (accumulation){accumulation tape \\
				\begin{tabular}{c|c|c}
					\hline
					$\dots$
					& \footnotesize $\sum_{i=1}^{j}\mathcal{T}_j(x)$ 
					& $\dots$ \\\hline
				\end{tabular}
			};
			
			%% 0. INITIALIZE
				\draw[->,rwth-purple] ([xshift=-40pt]input.south east) to [out=270, in=180]([yshift=-5pt]counting.west);
				\draw[->,rwth-purple] ([xshift=-40pt]input.south east) to [out=270, in=150]([yshift=0pt]output.west);
				
				\node[
				left=10pt of counting.west,
				yshift=-20pt,
				font=\tiny,rwth-purple
				]  {0. Initialize \\ $k:=n$ \\ $x:=n$};
			
			
			
			%% WHILE
				\node[below=-2pt of counting.south,xshift=20pt,rwth-purple] (while){$\circlearrowright$ {\tiny while $k>0$}};
				\node[below=-2pt of while.south, xshift=5pt,font=\tiny,rwth-purple] (decr) {$k$\texttt{-=}$1$};
				
				%% PROVIDE AS INPUT
				\draw[->,rwth-purple] ([yshift=-5pt]output.west) to [out=180,in=0]([yshift=-5pt]computation.east);
				\node[
				yshift=-25pt,
				right=5pt of computation.east,
				font=\tiny,rwth-purple
				] (provide) {1. Provide $x$ as \\input to each simulation};
				\draw[-,dashed,color=rwth-storm] ([xshift=20pt]input.south) -- ++(0,-50pt) node[midway,right,font=\tiny] {fetch description \\ and arity \\ of each $\mathcal{T}_i$};
			
			
			%% ADD TO ACCUMULATION
				\draw[->,rwth-purple] ([xshift=-5pt]computation.south) to [out=240, in=120]([xshift=-5pt]accumulation.north);
				\node[
				above=5pt of accumulation.north,
				xshift=-40pt,
				font=\tiny,
				rwth-purple] (add) {2. Add outputs to};
			
			
			%% WRITE SUM TO
				\draw[->,rwth-purple] (accumulation.south) to [out=330, in=210](output.south);
				\node[
				right=10pt of accumulation.south east,
				yshift=-15pt,
				font=\tiny,rwth-purple
				] (overwrite) {3. Overwrite $x$ with sum};
			
		\end{tikzpicture}
	\end{center}
	\caption{Uniform Turing Machine that computes the recursive completion.}
	\label{fig:uniform-tm-recursive-completion}
\end{figure}
Because the Turing Machine $\mathcal{T}_{\lozenge}$ regards $\tau$ merely as one of its inputs, it has a constant description length across all possible function sets $\tau$.
Together with Corollary \ref{cor:scaled-recursive-completion-not-non-recursive-expressible-placeholder}, this Theorem demonstrates that the structural limitation of non-recursive models can not be arbitrarily mended by appropriate choices of the underlying function set, including activation functions.
No matter how expressive the underlying function set is, the conditional Kolmogorov complexity of functions that cannot even be expressed by such models is upper bounded by a constant.

The corollary below joins these two results together.
% for the class of multi-layer neural networks, which constitute a subclass of non-recursive models.



% Specify what models are affected by this theorem.
\begin{corollary}[Simple Functions that non-recursive Models cannot Express]
	\label{cor:recursive-completion-models-cannot-express}
	There exists a constant $c\in\mathbb{N}$ such that the following holds:
	
	Given an arbitrary function set $\tau=\{f_1,\dots,f_j\}$, such that each $f_i$ is firstly partial computable and secondly strictly monotonously increasing or bounded. Assume that there is at least one $f_i\in\tau$ that is strictly monotonously increasing and has arity $\operatorname{ar}(f_i)>1$, e.g. the addition function.
	Denote the encoding $\operatorname{enc}(\tau)$ just as in Theorem \ref{theorem:recursive-completion-kolmogorov-complexity}.
	
	For any $m\in\mathbb{N}$, there are $\sqrt{2}^{m-c}-1$ functions $h:\mathbb{N}\to\mathbb{N}$ with a conditional Kolmogorov complexity 
	\begin{equation*}
		K\bigl(h\mid \operatorname{enc}(\tau)\bigr)\leq m
	\end{equation*}
	that cannot be expressed by any non-recursive model over $\tau$.
	% and therefore constructs a term $t_M\in T(\tau,\{x_1,\dots,x_k\})$ with $M^{-}(n)=t_M^{-}(n)$ for all $n\in\mathbb{N}$.
	
	% More precisely, for any such model $M$, there is a threshold $n_0(M)$ that is independent of $m$, such that for any of these functions $h$ and any $n\geq n_0$, $M(n)\neq h(n)$.
	% This threshold $n_0(M)$ solely depends linearly on the depth $\operatorname{dep}(t_M)$ of the term $t_M$ that $M$ coincides with. 
	% That's wrong, it also depends on the constants in our proof!
\end{corollary}
\begin{proof}
	By Corollary \ref{cor:scaled-recursive-completion-not-non-recursive-expressible-placeholder}, the function $\left(f_\tau\right)_a^{\lozenge}\notin\mathcal{F}_{\tau}$ for any $a\geq 1$.
	At the same time, Theorem \ref{theorem:recursive-completion-kolmogorov-complexity} ensures that there is a constant $c$ such that $K(\left(f_{\tau}\right)_{a}^{\lozenge}\mid \operatorname{enc}(\tau))\leq c+2\cdot\log_2(a)$ for any $a\in \mathbb{N}$.
	
	Let $m\in\mathbb{N}$ be arbitrary.
	For any $a\in\mathbb{N}$ with $a\leq \sqrt{2}^{m-c}$, $c+2\cdot\log_2(a)\leq  c + 2 \cdot \frac{1}{2}(m-c)\leq m$.
	Excluding $a=0$ yields our result.
\end{proof}

If we want to enable our models to express functions up to a certain simplicity, we must extend the structure of our model classes by recursive elements.
But even if our models were expressive enough to capture recursive functions like the recursive completion $\left(f_{\tau}\right)_a^{\lozenge}$, they are yet provided no incentive to favour them over other, possibly non-recursive hypotheses $h$.

In the next subsection, we will see that for any finite dataset, there are infinitely many non-recursive functions $f\in\mathcal{F}_{\tau}$ that remain perfectly consistent with $\left(f_{\tau}\right)_a^{\lozenge}$ and therefore achieve the optimal score in the IRM objective when the expected risks $R$ are accordingly replaced by the empirical risks $\hat{R}$ as is usually done in practice.
Consequently, non-recursive models over $\tau$ can indeed receive an optimal score although

Although no finite dataset can eliminate these optimal yet non-recursive functions, it will be shown that beyond some size threshold $n_0$, any function $f\in\mathcal{F}_{\tau}$ that still remains consistent with a finite dataset $S$ of size $|S|\geq n_0$ must have a higher conditional Kolmogorov complexity than the true function $\left(f_{\tau}\right)_a^{\lozenge}$.
For that reason, when optimization objectives additionally evaluated functions by their Kolmogorov complexity, sufficiently large finite datasets would in fact distinguish the true function $\left(f_{\tau}\right)_a^{\lozenge}$ from non-recursive functions.

\section{Optimization objectives lack a simplicity bias}
\label{sec:optimization-objectives-lack-simplicity-bias}
With full access to the underlying distribution $P$ by which problem instances $n\in\mathbb{N}$ occur, the expected risk $R$ can certainly identify the true function $\left(f_{\tau}\right)_a^{\lozenge}$ as its unique minimizer.
If any $n\in\mathbb{N}$ had a non-zero probability according to $P$, then any function that disagrees on at least one instance with $\left(f_{\tau}\right)_a^{\lozenge}$ would obtain a non-zero risk $R$. 
In such a case of perfect knowledge of $P$, even the standard ERM principle would suffice to identify the true function.
In practice however, datasets are bound to finiteness and the empirical risk $\hat{R}$ hence leaves functions infinitely many degrees of freedom as it merely comes to fitting the samples.
These degrees of freedom are either effaced by restrictions on the model class or further inductive biases in the optimization formulation.

After the last subsections addressed the former, this subsection demonstrates that inductive biases such as the simplicity bias towards minimal Kolmogorov complexity can further filter out a huge fraction of alternative hypotheses that would otherwise receive the same optimal optimization function value.

Firstly, for realistic assumptions on what functions the function set $\tau$ comprises, any finite dataset $S$ leaves infinitely many functions that non-recursive models could express to basically memorize the samples in $S$ and hence achieve optimal risk.
Even domain generalization optimization objectives like IRM can by itself not distinguish the recursive completion $\left(f_{\tau}\right)_a^{\lozenge}$ from these non-recursive functions $f\in \mathcal{F}_{\tau}$.
% Therefore, they all achieve the same optimal value in the IRM objective where the expected risks $R$ are approximated with empirical risks $\hat{R}$.

Complementing this result, we prove that all non-recursive functions eventually render suboptimal when we further take into account their Kolmogorov complexity.
Figure \ref{fig:eliminating-non-rec-funcs-simplicity-bias} underscores the discriminative power that Kolmogorov complexity or any similar simplicity bias exerts on the functions that remain consistent with the dataset.
Because the mere requirement of fitting samples leaves infinitely many degrees of freedom, optimization objectives like ERM assign infinitely many non-recursive functions $f\in\mathcal{F}_{\tau}$ the same optimal score for any finite dataset generated by $f_{\tau}^{\lozenge}$ (cf. Figure \ref{fig:no-simplicity-bias-perspective}).
On the other hand, only finitely many non-recursive functions $f\in\mathcal{F}_{\tau}$ remain at least as optimal as the true function if functions are additionally evaluated by a simplicity bias like their Kolmogorov complexity (cf. Figure \ref{fig:simplicity-bias-perspective}).

Over and above, there exists a threshold $n_0$ such that for \textit{any} dataset $S$ of size $|S|\geq n_0$, even these finitely many more optimal alternatives are eliminated, because any non-recursive function $f\in\mathcal{F}_{\tau}$ that still remains consistent with $S$ must have a higher conditional Kolmogorov complexity $K\bigl(f\mid \operatorname{enc}(\tau)\bigr)$ than the true function $\left(f_{\tau}\right)_a^{\lozenge}$.

\begin{lemma}[IRM does not Identify Recursive Completion]
	\label{lemma:recursive-completion-irm-cannot-identify}
	Let $\tau:=\{f_1,\dots,f_j\}$ be an arbitrary function set.
	Let $a\in\mathbb{N}$ with $a\geq 1$ be arbitrary.
	Assume that the functions below are included in $\tau$:
	\begin{enumerate}
		\item The addition function $f_{+}(x,y)=x+y$,
		\item The multiplication function $f_{\times}(x,y)=x\cdot y$,
		\item The equality function $f_{=}(x,y)=\begin{cases}
			1, & x=y\\
			0, & x\neq y
		\end{cases}$,
		\item The constants $c_1=1$, and $c_0=0$.
	\end{enumerate}
	
	For every finite dataset $S=\bigl\{\bigl(x_1,\left(f_{\tau}\right)_a^{\lozenge}(x_1)\bigr),\dots,\bigl(x_k,\left(f_{\tau}\right)_a^{\lozenge}(x_k)\bigr)\bigr\}$, there is an infinite subset $F_S\subset \mathcal{F}_{\tau}$ of functions $f\in F_S$ that achieve the optimal score according to the IRM objective from Equation \ref{eq:irm-objective} with the 0-1-loss and empirical risks $\hat{R}$, but are not equivalent to $\left(f_{\tau}\right)_a^{\lozenge}$.
\end{lemma}
\begin{proof}
	Let $\tau$ be as above.
	Fix an arbitrary $a\in\mathbb{N}$ with $a\geq 1$.
	
	We show that we can construct functions that memorize arbitrarily many samples.
	
	To begin with, any $n\in\mathbb{N}$ can be expressed by a constant function $c_n\in\mathcal{F}_{\tau}$. This is proved in a straightforward inductive manner.
	For the base case, we already have $c_0=0\in\tau$.
	Now, assume that there is an $n\in\mathbb{N}$ such that there exists a function $c_n\in\mathcal{F}_{\tau}$ with $c_n=n$.
	Then, we construct $c_{n+1}=f_{+}(c_n,c_1)=n+1$.
	By the inductive definition of non-recursive functions in \ref{def:non-recursive-functions}, it holds that $c_{n+1}\in \mathcal{F}_{\tau}$, and the overall statement draws on the induction principle.
	In the following, we will therefore directly write $n$ instead of $c_n$ for notational simplicity.
	
	Thereby, for any $m\in\mathbb{N}$, we can construct a function $f_m\in\mathcal{F}_{\tau}$ that memorizes the values of $\left(f_{\tau}\right)_a^{\lozenge}$ on $\{0,\dots,m\}$.
	This statement is analogously proved by induction over $m\in\mathbb{N}$.
	For the base case $m=0$, we define $f_0(n):=f_{\times}\bigl(f_{=}(0,n),\left(f_{\tau}\right)_a^{\lozenge}(0)\bigr)=\begin{cases}
		\left(f_{\tau}\right)_a^{\lozenge}(0), & n=0\\
		0, & n\neq 0
	\end{cases}$.
	
	As $f_0$ is composed of building block functions in $\tau$, it holds that $f_0\in\mathcal{F}_{\tau}$.
	
	But since $f_{+}\in\tau$ is strictly monotonously increasing with arity $\operatorname{ar}(f_{+})>1$, Lemmas \ref{lemma:self-lower-bound-strictly-monotonous-functions} and \ref{lemma:max-bound-recursive-concatenation-sum} ensure that
	\begin{align}
		\label{eq:lemma-irm-cannot-identify-self-bound-recursive-completion}
		\left(f_{\tau}\right)_a^{\lozenge}(n)&\overset{\ref{lemma:max-bound-recursive-concatenation-sum}}{\geq} f_{+}^{(n)}(n) \overset{\ref{lemma:self-lower-bound-strictly-monotonous-functions}}{\geq} n > 0 && \text{ for all } n\geq 1.
	\end{align}
	For that reason, $f_0(n)=\left(f_{\tau}\right)_a^{\lozenge}(n)$ only holds for $n=0$.
	
	Moving on with the induction hypothesis (IH), assume that there is an $m\in\mathbb{N}$ and a function $f_m\in\mathcal{F}_{\tau}$ such that $f_m(n)=\left(f_{\tau}\right)_a^{\lozenge}(n)$ holds for all $n\leq m$, but $f_m(n)=0$ for $n>m$.
	
	Then, we define
	\begin{align}
		f_{m+1}(n):=f_{+}\biggl(&\\
		& f_{\times}\left(f_{=}(m+1,n),\left(f_{\tau}\right)_a^{\lozenge}(m+1)\right), \\
		& f_m(n)\biggr).
	\end{align}
	Again, $f_{m+1}$ is a valid composition of functions in $\mathcal{F}_{\tau}$, thus $f_{m+1}\in\mathcal{F}_{\tau}$.
	
	By the induction hypothesis, it holds that
	\begin{align}
		f_{m+1}(n)=\begin{cases}
			\left(f_{\tau}\right)_a^{\lozenge}(m+1), & n=m+1\\
			f_m(n), & n\neq m+1
		\end{cases}\overset{\text{(IH)}}{=} \begin{cases}
			\left(f_{\tau}\right)_a^{\lozenge}(m+1), & n=m+1 \\
			\left(f_{\tau}\right)_a^{\lozenge}(n), & n\leq m\\
			0, & n > m+1
		\end{cases}.
	\end{align}
	
	Together with Equation \ref{eq:lemma-irm-cannot-identify-self-bound-recursive-completion}, we have $f_{m+1}(n)\neq \left(f_{\tau}\right)_a^{\lozenge}(n)$ for all $n>m+1$.
	
	By the induction principle, we therefore conclude that for any $m\in\mathbb{N}$ a function $f_m\in\mathcal{F}_{\tau}$ such that $f_m(n)=\left(f_{\tau}\right)_a^{\lozenge}(n)$ if and only if $n\leq m$.
	Collect these functions into $F:=\{f_m\mid m\in\mathbb{N}\}$.
	Because $f_m(m)=\left(f_{\tau}\right)_a^{\lozenge}(m)\overset{(\ref{eq:lemma-irm-cannot-identify-self-bound-recursive-completion})}{\neq}0=f_k(m)$ for all $k<m$, all these functions are pairwise different, and $|F|=\infty$.
	
	Now, let $\mathcal{P}_{tr}:=\{P_\varepsilon \mid  \varepsilon\in\mathcal{E}_{tr}\}$ be an arbitrary set of training distributions, $\mathcal{E}_{tr}\subset \mathcal{E}$.
	Let $S=\bigl\{\bigl(x_1,\left(f_{\tau}\right)_a^{\lozenge}(x_1)\bigr),\dots,\bigl(x_k,\left(f_{\tau}\right)_a^{\lozenge}(x_k)\bigr)\bigr\}$ be an arbitrary dataset of some size $k\in\mathbb{N}$ that summarizes all observed samples from $\mathcal{P}_{tr}$.
	The 0-1-loss is defined as 
	\begin{equation}
		\label{eq:0-1-loss}
		\ell(x,y)=\begin{cases}
			1, & x=y\\
			0, & x\neq y.
		\end{cases}
	\end{equation}
	
	Let $m_0:=\max_{1\leq i\leq k}x_i \in\mathbb{N}$ be the largest instance in $S$.
	For any $m\geq m_0$, it holds that $f_m(x_i)=\left(f_{\tau}\right)_a^{\lozenge}(x_i)$ for all $x_i$ in $S$.
	Therefore, the risks $\hat{R}_\varepsilon(f_m)=0$ are optimal for any training distribution $P_\varepsilon\in\mathcal{P}_{tr}$.
	There can be no function $f$ with $\hat{R}_{\varepsilon}(f)<\hat{R}_{\varepsilon}(f_m)$ for any $P_\varepsilon\in\mathcal{P}_{tr}$. 
	For that reason, $f_m$ is an optimal solution to Equation \ref{eq:irm-objective}.
	
	Choosing $F_S:=\{f_m\in F\mid m\geq m_0\}$ concludes our proof.
\end{proof}

Note that the function set $\tau':=\{f_{+},f_{\times},c_1,c_0,f_{=}\}$ that was assumed in Lemma \ref{lemma:recursive-completion-irm-cannot-identify} already satisfies the conditions of Theorem \ref{theorem:recursive-completion-not-non-recursive-expressible} for itself, since each function is either bounded or strictly monotonously increasing, and the addition function $f_{+}$ has arity $\operatorname{ar}(f_{+})>1$.
%This result can therefore be combined with the inexpressivity result of Theorem \ref{theorem:recursive-completion-not-non-recursive-expressible}.
Although any non-recursive model can therefore not express $\left(f_{\tau}\right)_a^{\lozenge}$, there are arbitrarily many non-recursive functions such models could draw upon to achieve an optimal risk by merely memorizing information.

Theorem \ref{theorem:recursive-completion-not-non-recursive-expressible} even paves the way to a stronger result that assures that \textit{any} non-recursive functions $f\in\mathcal{F}_{\tau}$ --- not only the ones explicitly constructed in the proof of Lemma \ref{lemma:recursive-completion-irm-cannot-identify} --- that agree with $\left(f_{\tau}\right)_a^{\lozenge}$ on sufficiently many points must obtain a higher Kolmogorov complexity than $\left(f_{\tau}\right)_a^{\lozenge}$.

If optimization objectives were to take into account this kind of simplicity bias, they could hence already filter out a huge amount of hypotheses despite the quantitative incompleteness of the training dataset.

\begin{theorem}[Surpassing the Kolmogorov Complexity Threshold]
	\label{theorem:surpass-kolmogorov-complexity-threshold}
	
	Let the function set $\tau$ be as in Theorem \ref{theorem:recursive-completion-not-non-recursive-expressible}.
	There is an $n_0\in\mathbb{N}$ such that for every dataset $S$ with size $|S|> n_0$, all functions $f\in \mathcal{F}_{\tau}$ that remain consistent with $S$ have a higher conditional Kolmogorov complexity than $\left(f_{\tau}\right)_a^{\lozenge}$,
	\begin{equation}
		K\bigl(f\mid \operatorname{enc}(\tau)\bigr) > K\bigl(\left(f_{\tau}\right)_a^{\lozenge} \mid \operatorname{enc}(\tau)\bigr).
	\end{equation}
\end{theorem}
\begin{proof}
	Let $a\in\mathbb{N}$ with $a\geq 1$ be arbitrary.
	By Theorem \ref{theorem:recursive-completion-kolmogorov-complexity}, the conditional Kolmogorov complexity $K\bigl(\left(f_{\tau}\right)_a^{\lozenge} \mid \operatorname{enc}(\tau)\bigr)\leq c+\log_2(a)$ for some $c\in\mathbb{N}$.
	
	Denote $c_0:=c+\log_2(a)$ in the following.
	By the geometric sum, there are only $2^{c_0+1}-1$ binary strings with a length shorter than or equal to $c_0$.
	
	Therefore, the set 
	\begin{equation}
		F:=\bigl\{f\in \mathcal{F}_{\tau} \mid K\bigl(f\mid \operatorname{enc}(\tau)\bigr)\leq K\bigl(\left(f_{\tau}\right)_a^{\lozenge} \mid \operatorname{enc}(\tau)\bigr)\bigr\}	
	\end{equation}
	must be finite.
	
	Because $F\subset \mathcal{F}_{\tau}$, Corollary \ref{cor:scaled-recursive-completion-not-non-recursive-expressible-placeholder} ensures that for each $f\in F$, there is an $n_0(f)$ such that $f(n)\neq \left(f_{\tau}\right)_a^{\lozenge}(n)$ for all $n\geq n_0(f)$.
	
	Let $n_0:=\max_{f\in F}n_0(f)$ be the maximum of these indices.
	
	Now, let $S=\bigl\{\bigl(x_1,\left(f_{\tau}\right)_a^{\lozenge}(x_1)\bigr),\dots,\bigl(x_k,\left(f_{\tau}\right)_a^{\lozenge}(x_k)\bigr)\bigr\}$ be an arbitrary dataset of size $k > n_0$.
	Denote by $m_1:=\max_{1\leq i\leq k}x_i$ the largest natural number instance in $S$.
	Since $|S|\geq n_0+1$, $m_1\geq n_0$.
	By the choice of $n_0$, we have $f(m_1)\neq \left(f_{\tau}\right)_a^{\lozenge}(m_1)$ for all $f\in F$.
	For any $f\in\mathcal{F}_{\tau}$ that is consistent with the functional pairs in $S$, it particularly holds that $f(m_1)=\left(f_{\tau}\right)_a^{\lozenge}(m_1)$.
	For that reason, $f\notin F$ and hence $K\bigl(f\mid \operatorname{enc}(\tau)\bigr) > K\bigl(\left(f_{\tau}\right)_a^{\lozenge} \mid \operatorname{enc}(\tau)\bigr)$.
\end{proof}
As soon as the dataset $D$ contains a sufficiently large instance $x_i\geq m_1$, any non-recursive function with a lower Kolmogorov complexity than $f_{\tau}^{\lozenge}$ must become inconsistent with $D$, since it maps $x_i$ to a smaller value than $f_{\tau}^{\lozenge}$ as was shown in the inexpressivity result of Theorem \ref{theorem:recursive-completion-not-non-recursive-expressible}.
Figure \ref{fig:one-sample-renders-suboptimal} visualizes this condition in the spirit of Figure \ref{fig:eliminating-non-rec-funcs-simplicity-bias}.
\begin{figure}[h]
	\raggedleft
	\begin{subfigure}{\textwidth}
		\raggedleft
		\begin{tikzpicture}[every text node part/.style={align=center}, font=\small]
			
			%%% ELLIPSES PERSPECTIVE
			
			% COMPUTABLE FUNCTIONS
			\node[ellipse, 
			draw=black, 
			fill=gray!20, 
			minimum height = 100pt, 
			minimum width=150pt,
			yshift=60pt] 
			(comp-ellipse) at (225pt, 0pt){};
			
			% NON-RECURSIVE FUNCTIONS
			\node[ellipse, 
			draw=rwth-teal!60,
			fill=rwth-teal!30, 
			fill opacity=0.2,
			text opacity=1,
			minimum width = 100pt, 
			minimum height= 80pt,
			label={[anchor=south,above=0pt,text=rwth-teal]270:$\mathcal{F}_{\tau}$}] 
			(non-rec-funcs-ellipse) at ([yshift=-3pt]comp-ellipse.center)  {};
			
			% CONSISTENT NON-RECURSIVE FUNCTIONS
			
			\node[ellipse, 
			draw=rwth-teal!80,
			fill=rwth-teal!80, 
			fill opacity=0.4,
			text opacity=1,
			minimum width = 80pt, 
			minimum height= 40pt,
			font=\tiny,
			text=white,
			label={[anchor=south,above=2pt,font=\tiny,text=white]270:consistent with $D$ \\ generated by $f_{\tau}^{\lozenge}$}] 
			(consistent-non-rec-funcs-ellipse) at ([yshift=0pt]non-rec-funcs-ellipse.center)  {}; 
			
			% INFINITE REMARK
			\path[draw=rwth-teal, dashed] ([yshift=-10pt]consistent-non-rec-funcs-ellipse.north) -- ++(5pt,50pt) node [above,font=\footnotesize,text=rwth-teal] {infinite};
			
			
			%%% LEGEND
			% PARTIAL COMPUTABLE
			\draw[yshift=60pt, fill=gray!20, font=\tiny] [xshift=20pt,yshift=-10pt](10:300pt) rectangle ++(5pt,5pt) node[right,yshift=-2pt] (legend-comp) {computable functions};
			
			
			% NON-RECURSIVE FUNCTIONS
			\draw[yshift=60pt, 
			draw=rwth-teal!60,
			fill=rwth-teal!30, 
			fill opacity=0.2,
			text opacity=1,
			font=\tiny] [xshift=20pt,yshift=-30pt](10:300pt) rectangle ++(5pt,5pt) node[right,yshift=-2pt] (legend-non-rec) {non-recursive functions};
			
			% CONSISTENT NON-REC FUNCTIONS
			\draw[yshift=60pt, 
			draw=rwth-teal!80,
			fill=rwth-teal!80, 
			fill opacity=0.4,
			text opacity=1,
			font=\tiny] [xshift=20pt,yshift=-40pt](10:300pt) rectangle ++(5pt,5pt) node[right,yshift=-2pt] (legend-non-rec) {consistent non-rec. functions};
			
			
			
			%% RECURSIVE COMPLETION IN ELLIPSE
			%\path[yshift=60pt, fill=black!80, draw=none] (170pt,25pt) circle [radius=2pt] node[ right] {$f_{\tau}^{\lozenge}$};
			
			%%% DATASET COORDINATE SYSTEM
			
			% AXES
			\node[yshift=60pt,xshift=35pt] (dataset-cs-center) at (-10:300pt) {};
			\path[yshift=60pt, draw, ->] 
			[xshift=30pt](-10:300pt) -- ++(50pt,0pt);
			\path[yshift=60pt, draw, ->] 
			[xshift=35pt,yshift=-5pt](-10:300pt) -- ++(0pt,50pt);
			
			% DATASET 1
			\path[draw=rwth-storm!80,
			fill=rwth-storm!30] 
			([xshift=30pt,yshift=20pt]dataset-cs-center) ellipse[x radius=15pt, y radius=10pt] node[text=black!80,font=\tiny] {$D$};
			
			
			%% RECURSIVE COMPLETION
			\path[yshift=60pt, fill=black!80, draw=none] (170pt,25pt) circle [radius=2pt] node[ right] {$f_{\tau}^{\lozenge}$};
			
		\end{tikzpicture}	
		\caption{Without any inductive bias beyond merely fitting the dataset, infinitely many non-recursive functions remain consistent and hence optimal according to optimization objectives like ERM.}
		\label{fig:no-simplicity-bias-perspective}
	\end{subfigure}\\
	\begin{subfigure}{\textwidth}
		\raggedleft
		\begin{tikzpicture}[every text node part/.style={align=center}, font=\small]
			
			%%% LEGEND
			% PARTIAL COMPUTABLE
			\draw[yshift=60pt, fill=gray!20, font=\tiny] [xshift=20pt,yshift=-10pt](10:300pt) rectangle ++(5pt,5pt) node[right,yshift=-2pt] (legend-comp) {computable functions};
			
			% SIMPLER PARTIAL COMPUTABLE
			\path[yshift=60pt, 
			draw,
			fill=gray!20, 
			font=\tiny] [xshift=20pt,yshift=-20pt] (10:300pt) rectangle ++(5pt,5pt)
			node[right,yshift=-2pt] (legend-simp) {simpler computable functions} ;
			
			% SEPARATE PATTERN
			\path[yshift=60pt, 
			pattern={Lines[angle=45, distance=2pt]}, 
			pattern color=Mahogany!60, 
			draw=none,
			font=\tiny] [xshift=20pt,yshift=-20pt](10:300pt) -- ++(5pt,0pt) -- ++ (0pt,5pt) -- ++(-5pt,0pt) -- cycle;
			
			
			% NON-RECURSIVE FUNCTIONS
			\draw[yshift=60pt, 
			draw=rwth-teal!60,
			fill=rwth-teal!30, 
			fill opacity=0.2,
			text opacity=1,
			font=\tiny] [xshift=20pt,yshift=-30pt](10:300pt) rectangle ++(5pt,5pt) node[right,yshift=-2pt] (legend-non-rec) {non-recursive functions};
			
			% CONSISTENT NON-REC FUNCTIONS
			\draw[yshift=60pt, 
			draw=rwth-teal!80,
			fill=rwth-teal!80, 
			fill opacity=0.4,
			text opacity=1,
			font=\tiny] [xshift=20pt,yshift=-40pt](10:300pt) rectangle ++(5pt,5pt) node[right,yshift=-2pt] (legend-non-rec) {consistent non-rec. functions};
			
			
			
			%% RECURSIVE COMPLETION IN ELLIPSE
			%\path[yshift=60pt, fill=black!80, draw=none] (170pt,25pt) circle [radius=2pt] node[ right] {$f_{\tau}^{\lozenge}$};
			
			%%% DATASET COORDINATE SYSTEM
			
			% AXES
			\node[yshift=60pt,xshift=35pt] (dataset-cs-center) at (-10:300pt) {};
			\path[yshift=60pt, draw, ->] 
			[xshift=30pt](-10:300pt) -- ++(50pt,0pt);
			\path[yshift=60pt, draw, ->] 
			[xshift=35pt,yshift=-5pt](-10:300pt) -- ++(0pt,50pt);
			
			% DATASET 1
			\path[draw=rwth-storm!80,
			fill=rwth-storm!30] 
			([xshift=30pt,yshift=20pt]dataset-cs-center) ellipse[x radius=15pt, y radius=10pt] node[text=black!80,font=\tiny] {$D$};
			
			
			
			%%% SIMPLICITY BIAS PERSPECTIVE
			
			%% KOLMOGOROV COMPLEXITY AXIS
			\draw[->] (0,0pt) -- (310pt,0pt) node[midway,xshift=100pt,below] {Kolmogorov complexity};
			
			%% PARTIAL COMPUTABLE FUNCTIONS
			\path[fill=gray!20, yshift=60pt,draw=none,rotate=-10] 
			(0pt,0pt) 
			-- (300pt,0pt) 
			arc [start angle=0, end angle=20, radius=300pt]
			-- cycle;
			\draw[yshift=60pt] (0pt,0pt) -- (10:300pt);
			\draw[yshift=60pt] (0pt,0pt) -- (-10:300pt); 
			
			%% SIMPLER FUNCTIONS
			\path[yshift=60pt, 
			pattern={Lines[angle=45, distance=5pt]}, 
			pattern color=Mahogany!60,
			draw=none] (0pt,0pt) -- (170pt,30pt) -- (170pt,-30pt) -- cycle;
			\path[yshift=60pt, draw=Mahogany!80, thick, dashed] (170pt,35pt) -- (170pt,-35pt);
			
			% BACKGROUND FOR F_TAU
			\node[yshift=60pt,fill=gray!20,circle,radius=7pt,minimum size=14pt] (non-rec-funcs-bg) at (110pt,-2pt) {};
			
			%% NON-RECURSIVE FUNCTIONS
			% ALL 
			\path[yshift=60pt, draw=rwth-teal!60] (6:300pt) .. controls (8:20pt) and (350:20pt) .. (352:300pt);
			\path[yshift=60pt, draw=none, fill=rwth-teal!30, fill opacity=0.2] (6:300pt) .. controls (8:20pt) and (350:20pt) .. (352:300pt) arc [start angle = -8, end angle = 6, radius=300pt];
			\node[yshift=60pt, text=rwth-teal!80] (non-rec-funcs) at (110pt,-2pt) {$\mathcal{F}_{\tau}$};
			
			% FINITENESS REMARKS
			\path[draw=Mahogany!80,decorate, decoration={brace,amplitude=10pt},yshift=65pt] (0pt,0pt) -- (10:170pt) node [midway, above,yshift=10pt,rotate=10,font=\footnotesize,text=Mahogany!80] {finite};
			\path[draw=gray,decorate, decoration={brace, amplitude=10pt},yshift=65pt] (10:175pt) -- (10:300pt) node [midway, yshift=10pt,above,rotate=10,font=\footnotesize,text=gray] (inf) {infinite};
			
			% CONSISTENT 1
			\path[yshift=60pt, draw=rwth-teal!80] (4:300pt) .. controls (8:60pt) and (350:60pt) .. (354:300pt);
			\path[yshift=60pt, draw=none, fill=rwth-teal!80, fill opacity=0.4] (4:300pt) .. controls (8:60pt) and (350:60pt) .. (354:300pt) arc [start angle = -6, end angle = 4, radius=300pt];
			\node[yshift=60pt, text=white, font=\tiny] (cons1) at (220pt,-5pt){consistent with $D$ \\ generated by $f_{\tau}^{\lozenge}$}; 
			
			
			%% RECURSIVE COMPLETION
			\path[yshift=60pt, fill=black!80, draw=none] (170pt,25pt) circle [radius=2pt] node[ right] {$f_{\tau}^{\lozenge}$};
			
		\end{tikzpicture}	
		\caption{Additionally exerting Kolmogorov complexity as a simplicity bias on the consistent functions induces an ordering on these consistent functions, leaving at most finitely many non-recursive functions with a better score than $f_{\tau}^{\lozenge}$.}
		\label{fig:simplicity-bias-perspective}
	\end{subfigure}
	
	\caption[The discriminate power of a Kolmogorov complexity bias beyond merely fitting a dataset.]{Juxtaposing the discriminate power of a Kolmogorov complexity bias beyond fitting a dataset with the indifference of standard optimization objectives like ERM and IRM to functions that perfectly fit the dataset.}
	\label{fig:eliminating-non-rec-funcs-simplicity-bias}
\end{figure}

The final message of this chapter can therefore be summarized as follows.
If we want to enable algorithms to learn simple functions, we not only need to bestow our machine learning models with recursive structure, but also need to extend our optimization objectives to favour simpler functions over more complex ones.

\begin{figure}[h]
	\raggedleft
	\begin{tikzpicture}[every text node part/.style={align=center}, font=\small]
		
		
		%%% LEGEND
		% PARTIAL COMPUTABLE
		\draw[yshift=60pt, fill=gray!20, font=\tiny] [xshift=20pt,yshift=-10pt](10:300pt) rectangle ++(5pt,5pt) node[right,yshift=-2pt] (legend-comp) {computable functions};
		
			% SIMPLER PARTIAL COMPUTABLE
			\path[yshift=60pt, 
			draw,
			fill=gray!20, 
			font=\tiny] [xshift=20pt,yshift=-20pt] (10:300pt) rectangle ++(5pt,5pt)
			node[right,yshift=-2pt] (legend-simp) {simpler computable functions} ;
			
			% SEPARATE PATTERN
			\path[yshift=60pt, 
			pattern={Lines[angle=45, distance=2pt]}, 
			pattern color=Mahogany!60, 
			draw=none,
			font=\tiny] [xshift=20pt,yshift=-20pt](10:300pt) -- ++(5pt,0pt) -- ++ (0pt,5pt) -- ++(-5pt,0pt) -- cycle;
		
		
		% NON-RECURSIVE FUNCTIONS
		\draw[yshift=60pt, 
		draw=rwth-teal!60,
		fill=rwth-teal!30, 
		fill opacity=0.2,
		text opacity=1,
		font=\tiny] [xshift=20pt,yshift=-30pt](10:300pt) rectangle ++(5pt,5pt) node[right,yshift=-2pt] (legend-non-rec) {non-recursive functions};
		
		% CONSISTENT NON-REC FUNCTIONS
			\draw[yshift=60pt, 
			draw=rwth-teal!80,
			fill=rwth-teal!80, 
			fill opacity=0.4,
			text opacity=1,
			font=\tiny] [xshift=20pt,yshift=-40pt](10:300pt) rectangle ++(5pt,5pt) node[right,yshift=-2pt] (legend-non-rec) {consistent non-rec. functions};
		
		
		
		%% RECURSIVE COMPLETION IN ELLIPSE
		%\path[yshift=60pt, fill=black!80, draw=none] (170pt,25pt) circle [radius=2pt] node[ right] {$f_{\tau}^{\lozenge}$};
		
		%%% DATASET COORDINATE SYSTEM
			
			% AXES
			\node[yshift=60pt,xshift=35pt] (dataset-cs-center) at (-10:300pt) {};
			\path[yshift=60pt, draw, ->] 
			[xshift=30pt](-10:300pt) -- ++(50pt,0pt);
			\path[yshift=60pt, draw, ->] 
			[xshift=35pt,yshift=-5pt](-10:300pt) -- ++(0pt,50pt);
			
			
			% DATASET 3
				\path[draw=rwth-storm!80,
				fill=rwth-storm!30] 
				([xshift=30pt,yshift=20pt]dataset-cs-center) ellipse[x radius=25pt, y radius=20pt] node[text=black!80,font=\tiny] {$D$};
			
			
			% M CIRCLE
				\path[fill=black!80] ([xshift=45pt,yshift=35pt]dataset-cs-center)circle[radius=2pt] node[right,font=\tiny]{$m_1$};
			
		
		
		%%% SIMPLICITY BIAS PERSPECTIVE
			
			%% KOLMOGOROV COMPLEXITY AXIS
			\draw[->] (0,0pt) -- (310pt,0pt) node[midway,xshift=100pt,below] {Kolmogorov complexity};
			
			%% PARTIAL COMPUTABLE FUNCTIONS
			\path[fill=gray!20, yshift=60pt,draw=none,rotate=-10] 
			(0pt,0pt) 
			-- (300pt,0pt) 
			arc [start angle=0, end angle=20, radius=300pt]
			-- cycle;
			\draw[yshift=60pt] (0pt,0pt) -- (10:300pt);
			\draw[yshift=60pt] (0pt,0pt) -- (-10:300pt); 
			
			%% SIMPLER FUNCTIONS
			\path[yshift=60pt, 
			pattern={Lines[angle=45, distance=5pt]}, 
			pattern color=Mahogany!60,
			draw=none] (0pt,0pt) -- (170pt,30pt) -- (170pt,-30pt) -- cycle;
			\path[yshift=60pt, draw=Mahogany!80, thick, dashed] (170pt,35pt) -- (170pt,-35pt);
			
			% BACKGROUND FOR F_TAU
			\node[yshift=60pt,fill=gray!20,circle,radius=7pt,minimum size=14pt] (non-rec-funcs-bg) at (110pt,-2pt) {};
			
			%% NON-RECURSIVE FUNCTIONS
			% ALL 
			\path[yshift=60pt, draw=rwth-teal!60] (6:300pt) .. controls (8:20pt) and (350:20pt) .. (352:300pt);
			\path[yshift=60pt, draw=none, fill=rwth-teal!30, fill opacity=0.2] (6:300pt) .. controls (8:20pt) and (350:20pt) .. (352:300pt) arc [start angle = -8, end angle = 6, radius=300pt];
			\node[yshift=60pt, text=rwth-teal!80] (non-rec-funcs) at (110pt,-2pt) {$\mathcal{F}_{\tau}$};
			
			
			% CONSISTENT 3
				\path[yshift=60pt, draw=rwth-teal!80] (4:300pt) .. controls (8:140pt) and (350:140pt) .. (354:300pt);
				\path[yshift=60pt, draw=none, fill=rwth-teal!80, fill opacity=0.4] (4:300pt) .. controls (8:140pt) and (350:140pt) .. (354:300pt) arc [start angle = -6, end angle = 4, radius=300pt];
				\node[yshift=60pt, text=white, font=\tiny] (cons1) at (240pt,-5pt){consistent with $D$ \\ generated by $f_{\tau}^{\lozenge}$};
			
		
		
		%% RECURSIVE COMPLETION
		\path[yshift=60pt, fill=black!80, draw=none] (170pt,25pt) circle [radius=2pt] node[ right] {$f_{\tau}^{\lozenge}$};
		
	\end{tikzpicture}	
	\caption[One sufficiently large sample renders all consistent non-recursive functions suboptimal.]{One sufficiently large sample renders all consistent non-recursive functions suboptimal. Here, $m_1$ is the threshold in the proof of Theorem \ref{theorem:surpass-kolmogorov-complexity-threshold}, above which every non-recursive functions with a lower Kolmogorov complexity than $f_{\tau}^{\lozenge}$ must become inconsistent by Theorem \ref{theorem:recursive-completion-not-non-recursive-expressible}.}
	\label{fig:one-sample-renders-suboptimal}
\end{figure}
It remains to ask how such measures could alleviate the strong information-theoretic conditions that training distributions still have to satisfy to fall within the scope of generalization guarantees.
In the next chapter, we demonstrate how Kolmogorov complexity paves the way to formulate such sufficient conditions in a general, but still nearly optimal manner.
To that end, it first exhibits that it is not meaningful to condition learnability on the number of samples in dataset. 
Instead, it proposes to measure the information that a dataset conveys about the functions that could have generated it in terms of Kolmogorov complexity, and demonstrates how this measure yields not only generalization guarantees, but could realise simplicity biases in practice.